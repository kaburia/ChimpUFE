{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Mcyx_MH6H2"
      },
      "source": [
        "# Dimensionality Reduction: Latent Variable Modelling\n",
        "\n",
        "### Neil D. Lawrence\n",
        "\n",
        "### 2025-09-22"
      ],
      "id": "e0Mcyx_MH6H2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8FC9pT8H6JD"
      },
      "source": [
        "**Abstract**: In this lecture we turn to *unsupervised learning*.\n",
        "Specifically, we introduce the idea of a latent variable model. Latent\n",
        "variable models are a probabilistic perspective on unsupervised learning\n",
        "which lead to dimensionality reduction algorithms."
      ],
      "id": "C8FC9pT8H6JD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dtBPolmH6JG"
      },
      "source": [
        "$$\n",
        "$$"
      ],
      "id": "0dtBPolmH6JG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e_BfzUMH6JI"
      },
      "source": [
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!---->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
        "<!--\n",
        "\n",
        "-->"
      ],
      "id": "-e_BfzUMH6JI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXILQBkyH6JJ"
      },
      "source": [
        "## ML Foundations Course Notebook Setup\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_mlfc/includes/mlfc-notebook-setup.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We install some bespoke codes for creating and saving plots as well as\n",
        "loading data sets."
      ],
      "id": "KXILQBkyH6JJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrpyzXDYH6JK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install notutils\n",
        "%pip install pods\n",
        "%pip install git+https://github.com/lawrennd/mlai.git"
      ],
      "id": "jrpyzXDYH6JK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU1eI0QrH6JP"
      },
      "outputs": [],
      "source": [
        "import notutils\n",
        "import pods\n",
        "import mlai\n",
        "import mlai.plot as plot"
      ],
      "id": "VU1eI0QrH6JP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfH04z-YH6JS"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.size': 22})"
      ],
      "id": "IfH04z-YH6JS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qR7HuMeH6JT"
      },
      "source": [
        "<!--setupplotcode{import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_context('paper')\n",
        "sns.set_palette('colorblind')}-->"
      ],
      "id": "2qR7HuMeH6JT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYsaBtK3H6JV"
      },
      "source": [
        "## Review\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/latent-variable-modelling.gpp.markdown\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/latent-variable-modelling.gpp.markdown', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "So far in our classes we have focussed on regression problems and\n",
        "generalised linear models. These are examples of supervised learning. We\n",
        "have considered the relationship between the likelihood and the\n",
        "objective function and we have shown how we can find paramters by\n",
        "maximizing the likelihood (equivalent to minimizing the objective\n",
        "function) in this session we look at latent variables."
      ],
      "id": "BYsaBtK3H6JV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmftmAoWH6JW"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/clustering.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/clustering.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Clustering is a common approach to data analysis, though we will not\n",
        "cover it in great depth in this course. The fundamental idea is to\n",
        "associate each data point $\\mathbf{ y}_{i, :}$ with one of $k$ different\n",
        "discrete groups. This approach raises interesting questions - for\n",
        "instance, when clustering animals into groups, we might ask whether\n",
        "animal traits are truly discrete or continuous in nature. Similar\n",
        "questions arise when clustering political affiliations.\n",
        "\n",
        "Humans seem to have a natural affinity for discrete clustering\n",
        "approaches. This makes clustering particularly useful when collaborating\n",
        "with biologists, who often think in terms of discrete categories.\n",
        "However, we should be mindful that this preference for discrete\n",
        "categories may sometimes oversimplify continuous variations in data.\n",
        "\n",
        "There is a subtle but important distinction between clustering and\n",
        "vector quantisation. In true clustering, we typically expect to see\n",
        "reductions in data density between natural groups - essentially, gaps in\n",
        "the data that separate different clusters. This definition isn’t\n",
        "universally applied though, and vector quantization may partition data\n",
        "without requiring such density gaps. For our current discussion, we’ll\n",
        "treat them similarly, focusing on the common challenges they share: how\n",
        "to allocate points to groups and, more challengingly, how to determine\n",
        "the optimal number of groups.\n",
        "\n",
        "Clustering methods associate data points with different labels that are\n",
        "allocated by the computer rather than provided by human annotators. This\n",
        "process is quite intuitive for humans - we naturally cluster our\n",
        "observations of the real world. For example, we cluster animals into\n",
        "groups like birds, mammals, and insects. While these labels can be\n",
        "provided by humans, they were originally invented through a clustering\n",
        "process. With computational clustering, we want to recreate that process\n",
        "of label invention.\n",
        "\n",
        "When thinking about ideas, the Greek philosopher Plato considered the\n",
        "concept of Platonic ideals - the most quintessential version of a thing,\n",
        "like the most bird-like bird or chair-like chair. In clustering, we aim\n",
        "to define different categories by finding their Platonic ideals (cluster\n",
        "centers) and allocating each data point to its nearest center. This\n",
        "allows computers to form categorizations of data at scales too large for\n",
        "human processing."
      ],
      "id": "nmftmAoWH6JW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chimpanzee Face Clustering\n",
        "\n",
        "We know that human faces are unique to each of us but did you know that chimpanzees also have unique faces and different images of the same chimpanzee would naturally cluster together?\n",
        "This is what we shall be doing with this notebook, using a sample set of 5 chimpanzees with a total of 25 images and see if we can be able to cluster the dataset\n",
        "- 000000 (5 images)\n",
        "- 000001 (5 images)\n",
        "- 000002 (5 images)\n",
        "- 000003 (5 images)\n",
        "- 000004 (5 images)\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Face Embedding Extraction**  \n",
        "   Facial embeddings are computed from chimpanzee face images using a pre-trained deep learning model. These embeddings encode high-level facial features in a numerical vector space.\n",
        "\n",
        "   *Iashin, Vladimir, et al. \"Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder.\" arXiv preprint arXiv:2507.10552 (2025).*\n",
        "\n",
        "\n",
        "2. **Clustering**  \n",
        "   K-Means clustering is applied on the embeddings to group similar faces together. This allows us to identify unique individuals or sets of visually similar faces.\n",
        "\n",
        "3. **Visualization**  \n",
        "   Dimensionality reduction techniques (e.g., PCA, t-SNE, or UMAP) are used to project the high-dimensional embeddings into 2D space. The plots clearly show that the faces of chimpanzees cluster, supporting the hypothesis that embeddings can capture individual identity.\n",
        "\n",
        "## Key Result\n",
        "\n",
        "The plots in the notebook reveal some distinct clusters corresponding to different chimpanzee individuals.\n",
        "\n",
        "1. Run all cells to reproduce the results:\n",
        "   - Extract embeddings  \n",
        "   - Apply clustering  \n",
        "   - Visualize clusters  \n",
        "\n",
        "2. Inspect the generated plots to confirm that chimpanzee faces group into meaningful clusters.\n",
        "\n",
        "## Applications\n",
        "\n",
        "- Wildlife monitoring and conservation  \n",
        "- Automated identification of individual animals  \n",
        "- Studying social behavior in primates  \n",
        "\n",
        "---\n",
        "\n",
        "### PS\n",
        "\n",
        "One thing to note is though *0.72%* cluster together some of the faces are not we ll clustered and different loss functions could be used to reduce the similarity distances and increase the dissimilarity\n",
        "\n",
        "<br>\n",
        "This number was obtained since we have the labels on the images to validate the clusters"
      ],
      "metadata": {
        "id": "MMMCCv0CO61u"
      },
      "id": "MMMCCv0CO61u"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kaburia/ChimpUFE.git\n",
        "!cd ChimpUFE && pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "gnQw6SyxO-_H"
      },
      "id": "gnQw6SyxO-_H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('ChimpUFE')"
      ],
      "metadata": {
        "id": "xR942arRPFWw"
      },
      "id": "xR942arRPFWw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import matplotlib.image as mpimg\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import notutils as nu\n",
        "import mlai"
      ],
      "metadata": {
        "id": "zQctHdotPJoh"
      },
      "id": "zQctHdotPJoh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we'd extract the pretrained weights of the pretrained model on chimpanzee faces"
      ],
      "metadata": {
        "id": "Xucpc-73PTAA"
      },
      "id": "Xucpc-73PTAA"
    },
    {
      "cell_type": "code",
      "source": [
        "# face recognition weights\n",
        "!wget -P ./assets/weights https://github.com/v-iashin/ChimpUFE/releases/download/v1.1/25-08-29T11-49-28_340k.pth"
      ],
      "metadata": {
        "id": "EoDn7YhiPN4Y"
      },
      "id": "EoDn7YhiPN4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cloned repository contains a gallery with the sample classess where we shall extract the embeddings for the images"
      ],
      "metadata": {
        "id": "u_xmWZQGPXga"
      },
      "id": "u_xmWZQGPXga"
    },
    {
      "cell_type": "code",
      "source": [
        "# different individuals\n",
        "!python demo_face_rec.py \\\n",
        "  --pretrained_weights ./assets/weights/25-08-29T11-49-28_340k.pth \\\n",
        "    --gallery_path ./assets/gallery \\\n",
        "    --embeddings embeddings.pt\n"
      ],
      "metadata": {
        "id": "LJGOcK82PYih"
      },
      "id": "LJGOcK82PYih",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the extracted embeddings\n",
        "data = torch.load(\"/content/ChimpUFE/embeddings.pt\")\n",
        "embeddings, paths = data[\"embeddings\"], data[\"paths\"]\n",
        "print(embeddings.shape)\n"
      ],
      "metadata": {
        "id": "IPwQ80byPdbX"
      },
      "id": "IPwQ80byPdbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmopWTsEH6Ja"
      },
      "source": [
        "## $k$-means Clustering\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/k-means-clustering.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/k-means-clustering.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "To implement clustering computationally, we need to mathematically\n",
        "represent both our objects and cluster centers as vectors\n",
        "($\\mathbf{ x}_i$ and $\\boldsymbol{ \\mu}_j$ respectively) and define a\n",
        "notion of either similarity or distance between them. The distance\n",
        "function $d_{ij} = f(\\mathbf{ x}_i, \\boldsymbol{ \\mu}_j)$ measures how\n",
        "far each object is from potential cluster centers. For example, we might\n",
        "cluster customers by representing them through their purchase history\n",
        "and measuring their distance to different customer archetypes."
      ],
      "id": "fmopWTsEH6Ja"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTYuPoXyH6Jb"
      },
      "source": [
        "## Squared Distance\n",
        "\n",
        "A commonly used distance metric is the squared distance:\n",
        "$d_{ij} = (\\mathbf{ x}_i - \\boldsymbol{ \\mu}_j)^2$. This metric appears\n",
        "frequently in machine learning - we saw it earlier measuring prediction\n",
        "errors in regression, and here it measures dissimilarity between data\n",
        "points and cluster centers.\n",
        "\n",
        "Once we have decided on the distance or similarity function, we can\n",
        "decide a number of cluster centers, $K$. We find their location by\n",
        "allocating each center to a sub-set of the points and minimizing the sum\n",
        "of the squared errors, $$\n",
        "E(\\mathbf{M}) = \\sum_{i \\in \\mathbf{i}_j} (\\mathbf{ x}_i - \\boldsymbol{ \\mu}_j)^2\n",
        "$$ where the notation $\\mathbf{i}_j$ represents all the indices of each\n",
        "data point which has been allocated to the $j$th cluster represented by\n",
        "the center $\\boldsymbol{ \\mu}_j$."
      ],
      "id": "vTYuPoXyH6Jb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFP-8SqNH6Jc"
      },
      "source": [
        "## $k$-Means Clustering\n",
        "\n",
        "One approach to minimizing this objective function is known as\n",
        "*$k$-means clustering*. It is simple and relatively quick to implement,\n",
        "but it is an initialization sensitive algorithm. Initialization is the\n",
        "process of choosing an initial set of parameters before optimization.\n",
        "For $k$-means clustering you need to choose an initial set of centers.\n",
        "In $k$-means clustering your final set of clusters is very sensitive to\n",
        "the initial choice of centers. For more technical details on $k$-means\n",
        "clustering you can watch a video of Alex Ihler introducing the algorithm\n",
        "here.\n",
        "\n",
        "The $k$-means algorithm provides a straightforward approach to\n",
        "clustering data. It requires two key elements: a set of $k$ cluster\n",
        "centres and a way to assign each data point to a cluster. The algorithm\n",
        "follows a simple iterative process:\n",
        "\n",
        "1.  First, initialize cluster centres by randomly selecting $k$ data\n",
        "    points\n",
        "2.  Assign each data point to its nearest cluster centre\n",
        "3.  Update each cluster centre by computing the mean of all points\n",
        "    assigned to it\n",
        "4.  Repeat steps 2 and 3 until the cluster assignments stop changing\n",
        "\n",
        "This process is intuitive and relatively easy to implement, though it\n",
        "comes with certain limitations.\n",
        "\n",
        "The $k$-means algorithm works by minimizing an objective function that\n",
        "measures the sum of squared Euclidean distances between each point and\n",
        "its assigned cluster center. This objective function can be written\n",
        "mathematically as shown above, where $\\boldsymbol{ \\mu}_{j, :}$\n",
        "represents the mean of cluster $j$.\n",
        "\n",
        "It’s important to understand that while this algorithm will always\n",
        "converge to a minimum, this minimum is not guaranteed to be either\n",
        "global or unique. The optimization problem is non-convex, meaning there\n",
        "can be multiple local minima. Different initializations of the cluster\n",
        "centers can lead to different final solutions, which is one of the key\n",
        "challenges in applying $k$-means clustering in practice."
      ],
      "id": "eFP-8SqNH6Jc"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Searching for the optimal k value and computing the k means clustering\n",
        "\n",
        "def cluster_kmeans(X, k=8, random_state=42):\n",
        "    km = KMeans(n_clusters=k, random_state=random_state)\n",
        "    labels = km.fit_predict(X)\n",
        "    centers = km.cluster_centers_  # shape (k, D)\n",
        "    return labels, centers\n",
        "\n",
        "def write_plot(counter, caption):\n",
        "    directory = \"./ml\"\n",
        "    filestub = f\"kmeans_clustering_{counter:0>3}\"\n",
        "    mlai.write_figure(filestub+\".svg\", directory=directory)\n",
        "    f = open(os.path.join(directory,filestub) + '.md', 'w')\n",
        "    f.write(caption)\n",
        "    f.close()\n",
        "\n",
        "def iterative_kmeans_widget(\n",
        "    embeddings,\n",
        "    paths=None,\n",
        "    k_min=2,\n",
        "    k_max=10,\n",
        "    max_iter=10,\n",
        "    fontsize=16,\n",
        "    counter_start=0,\n",
        "    draw_ellipse=False,\n",
        "    add_images=False,\n",
        "    image_zoom=0.2,\n",
        "    image_ratio=1.0,\n",
        "    use_true_clusters=False,\n",
        "    k_clusters=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Iterative K-means visualization with optional ellipses drawn for clusters\n",
        "    and optional chimpanzee face images placed on the plot.\n",
        "\n",
        "    embeddings: np.ndarray [N, D]\n",
        "    paths: list of str, file paths to chimpanzee face images (must align with embeddings)\n",
        "    image_ratio: fraction of images per cluster to overlay\n",
        "    use_true_clusters: if True, project actual high-dim clustering into 2D;\n",
        "                       if False, run toy KMeans in PCA 2D space for visualization\n",
        "    k_clusters: If None find the optimal number of clusters by the max silhouette scores\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Reduce to 2D for visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    Y = pca.fit_transform(embeddings)\n",
        "    if k_clusters is not None:\n",
        "        best_k = k_clusters\n",
        "\n",
        "    else:\n",
        "      # Step 2: Select optimal k using silhouette score\n",
        "      sil_scores = []\n",
        "      for k in range(k_min, k_max+1):\n",
        "          km = KMeans(n_clusters=k, n_init=max_iter, random_state=42).fit(embeddings)\n",
        "          score = silhouette_score(embeddings, km.labels_)\n",
        "          sil_scores.append((k, score))\n",
        "      best_k = max(sil_scores, key=lambda x: x[1])[0]\n",
        "      print(f\"Optimal number of clusters (silhouette): {best_k}\")\n",
        "\n",
        "    # Fit final clustering in high-dimensional space\n",
        "    final_kmeans = KMeans(n_clusters=best_k, n_init=max_iter, random_state=42).fit(embeddings)\n",
        "    final_labels = final_kmeans.labels_\n",
        "    final_centers = final_kmeans.cluster_centers_  # [best_k, D]\n",
        "\n",
        "    # Project cluster centers to 2D\n",
        "    projected_centers = pca.transform(final_centers)\n",
        "\n",
        "    # Plot setup\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    counter = counter_start\n",
        "    pi_vals = np.linspace(0, 2*np.pi, 200)\n",
        "\n",
        "    if not use_true_clusters:\n",
        "        centre_inds = np.random.permutation(Y.shape[0])[:best_k]\n",
        "        centres = Y[centre_inds, :]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            # Assign points in 2D\n",
        "            dist_mat = ((Y[:, :, None] - centres.T[None, :, :])**2).sum(1)\n",
        "            ind = dist_mat.argmin(1)\n",
        "\n",
        "            ax.cla()\n",
        "            colors = [\"green\", \"blue\", \"purple\", \"orange\", \"cyan\", \"magenta\"]\n",
        "            markers = [\"x\", \"o\", \"+\", \"s\", \"d\", \"^\", \"v\"]\n",
        "\n",
        "            for j in range(best_k):\n",
        "                cluster_points = Y[ind == j, :]\n",
        "\n",
        "                if not add_images:\n",
        "                    ax.plot(cluster_points[:, 0], cluster_points[:, 1],\n",
        "                            markers[j % len(markers)], color=colors[j % len(colors)],\n",
        "                            markersize=8, alpha=0.7)\n",
        "\n",
        "                # Subsample images if requested\n",
        "                if add_images and paths is not None:\n",
        "                    cluster_paths = np.array(paths)[ind == j]\n",
        "                    num_to_plot = max(1, int(len(cluster_paths) * image_ratio))\n",
        "                    chosen_paths = np.random.choice(cluster_paths, size=num_to_plot, replace=False)\n",
        "                    for (x0, y0), img_path in zip(cluster_points[:num_to_plot], chosen_paths):\n",
        "                        try:\n",
        "                            img = mpimg.imread(img_path)\n",
        "                            imagebox = OffsetImage(img, zoom=image_zoom)\n",
        "                            ab = AnnotationBbox(imagebox, (x0, y0), frameon=False)\n",
        "                            ax.add_artist(ab)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Could not load image {img_path}: {e}\")\n",
        "\n",
        "                if draw_ellipse and cluster_points.shape[0] > 2:\n",
        "                    cov = np.cov(cluster_points, rowvar=False)\n",
        "                    vals, vecs = np.linalg.eigh(cov)\n",
        "                    order = vals.argsort()[::-1]\n",
        "                    vals, vecs = vals[order], vecs[:, order]\n",
        "                    L = np.dot(vecs, np.diag(np.sqrt(vals)))\n",
        "                    ellipse = np.dot(np.column_stack([np.cos(pi_vals), np.sin(pi_vals)]), L.T)\n",
        "                    ax.plot(centres[j, 0] + ellipse[:, 0], centres[j, 1] + ellipse[:, 1],\n",
        "                            linewidth=2, color=\"black\")\n",
        "\n",
        "            # Plot centres\n",
        "            ax.plot(centres[:, 0], centres[:, 1], 'o', color=\"black\", markersize=12, linewidth=3)\n",
        "            ax.set_title(f\"K-means Iteration {i+1} (PCA space)\")\n",
        "            ax.set_xlabel(\"PC1\", fontsize=fontsize)\n",
        "            ax.set_ylabel(\"PC2\", fontsize=fontsize)\n",
        "\n",
        "            write_plot(counter, f\"Assign points to clusters (iteration {i+1}).\")\n",
        "            counter += 1\n",
        "\n",
        "            # Update centres\n",
        "            for j in range(best_k):\n",
        "                if np.any(ind == j):\n",
        "                    centres[j, :] = np.mean(Y[ind == j, :], axis=0)\n",
        "\n",
        "    else:\n",
        "        ax.cla()\n",
        "        colors = [\"green\", \"blue\", \"purple\", \"orange\", \"cyan\", \"magenta\"]\n",
        "        markers = [\"x\", \"o\", \"+\", \"s\", \"d\", \"^\", \"v\"]\n",
        "\n",
        "        for j in range(best_k):\n",
        "            cluster_points = Y[final_labels == j, :]\n",
        "\n",
        "            if not add_images:\n",
        "                ax.plot(cluster_points[:, 0], cluster_points[:, 1],\n",
        "                        markers[j % len(markers)], color=colors[j % len(colors)],\n",
        "                        markersize=8, alpha=0.7)\n",
        "\n",
        "            if add_images and paths is not None:\n",
        "                cluster_paths = np.array(paths)[final_labels == j]\n",
        "                num_to_plot = max(1, int(len(cluster_paths) * image_ratio))\n",
        "                chosen_paths = np.random.choice(cluster_paths, size=num_to_plot, replace=False)\n",
        "                for (x0, y0), img_path in zip(cluster_points[:num_to_plot], chosen_paths):\n",
        "                    try:\n",
        "                        img = mpimg.imread(img_path)\n",
        "                        imagebox = OffsetImage(img, zoom=image_zoom)\n",
        "                        ab = AnnotationBbox(imagebox, (x0, y0), frameon=False)\n",
        "                        ax.add_artist(ab)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Could not load image {img_path}: {e}\")\n",
        "\n",
        "            if draw_ellipse and cluster_points.shape[0] > 2:\n",
        "                cov = np.cov(cluster_points, rowvar=False)\n",
        "                vals, vecs = np.linalg.eigh(cov)\n",
        "                order = vals.argsort()[::-1]\n",
        "                vals, vecs = vals[order], vecs[:, order]\n",
        "                L = np.dot(vecs, np.diag(np.sqrt(vals)))\n",
        "                ellipse = np.dot(np.column_stack([np.cos(pi_vals), np.sin(pi_vals)]), L.T)\n",
        "                ax.plot(projected_centers[j, 0] + ellipse[:, 0],\n",
        "                        projected_centers[j, 1] + ellipse[:, 1],\n",
        "                        linewidth=2, color=\"black\")\n",
        "\n",
        "        # Plot true projected centres\n",
        "        ax.plot(projected_centers[:, 0], projected_centers[:, 1],\n",
        "                'o', color=\"black\", markersize=12, linewidth=3)\n",
        "        ax.set_title(\"K-means Clustering (true clusters projected to PCA)\")\n",
        "        ax.set_xlabel(\"PC1\", fontsize=fontsize)\n",
        "        ax.set_ylabel(\"PC2\", fontsize=fontsize)\n",
        "\n",
        "        write_plot(counter, \"Clusters visualized in PCA projection.\")\n",
        "        counter += 1\n",
        "\n",
        "    return counter, best_k, final_labels\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HV8wryU9Pkzb"
      },
      "id": "HV8wryU9Pkzb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iteration = 100\n",
        "counter, best_k, final_labels = iterative_kmeans_widget(embeddings,\n",
        "                        paths=paths, add_images=True,\n",
        "                        draw_ellipse=True,\n",
        "                        max_iter=max_iteration,\n",
        "                        image_zoom=0.2,\n",
        "                        image_ratio=0.4)"
      ],
      "metadata": {
        "id": "cv9DlFM9PrfO"
      },
      "id": "cv9DlFM9PrfO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nu.display_plots(\"kmeans_clustering_{counter:0>3}.svg\", directory=\"./ml\",\n",
        "                            text_top='kmeans_clustering_{counter:0>3}.tex', counter=(0, max_iteration-1))"
      ],
      "metadata": {
        "id": "ZrE8Q91CPwaE"
      },
      "id": "ZrE8Q91CPwaE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_clusters_by_paths(paths, cluster_labels):\n",
        "    \"\"\"\n",
        "    Validate clustering results against ground-truth IDs\n",
        "    encoded in the parent folder names of the file paths.\n",
        "\n",
        "    Args:\n",
        "        paths: list of str, file paths (e.g. \".../000000/01.png\")\n",
        "        cluster_labels: np.ndarray of shape [N,], cluster assignments from KMeans\n",
        "\n",
        "    Returns:\n",
        "        bad_mask: boolean array of shape [N,], True if misclustered\n",
        "        accuracy: float, overall clustering accuracy\n",
        "    \"\"\"\n",
        "    # Extract true labels from parent folder name\n",
        "    true_labels = [os.path.basename(os.path.dirname(p)) for p in paths]\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    # Map each ground-truth ID to its dominant cluster\n",
        "    bad_mask = np.zeros(len(paths), dtype=bool)\n",
        "    gt_to_cluster = {}\n",
        "\n",
        "    for gt in np.unique(true_labels):\n",
        "        idx = np.where(true_labels == gt)[0]\n",
        "        assigned_clusters = cluster_labels[idx]\n",
        "        if len(assigned_clusters) == 0:\n",
        "            continue\n",
        "        # majority cluster for this gt class\n",
        "        dominant_cluster = Counter(assigned_clusters).most_common(1)[0][0]\n",
        "        gt_to_cluster[gt] = dominant_cluster\n",
        "\n",
        "        # mark misclustered points\n",
        "        bad_mask[idx] = assigned_clusters != dominant_cluster\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = 1.0 - bad_mask.mean()\n",
        "\n",
        "    return bad_mask, accuracy\n",
        "\n",
        "bad_mask, accuracy = validate_clusters_by_paths(paths, final_labels)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "id": "v3namXq9P0J1"
      },
      "id": "v3namXq9P0J1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cvjena/chimpanzee_faces.git"
      ],
      "metadata": {
        "id": "e0O_62TfP4Yz"
      },
      "id": "e0O_62TfP4Yz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running through the additional dataset\n",
        "!python demo_face_rec.py \\\n",
        "  --pretrained_weights ./assets/weights/25-08-29T11-49-28_340k.pth \\\n",
        "    --gallery_path ./chimpanzee_faces/datasets_cropped_chimpanzee_faces/data_CTai \\\n",
        "    --embeddings new_dataset_embeddings.pt"
      ],
      "metadata": {
        "id": "eVPJPMR4P7HU"
      },
      "id": "eVPJPMR4P7HU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the embeddings\n",
        "data_ = torch.load(\"/content/ChimpUFE/new_dataset_embeddings.pt\")\n",
        "embeddings_data, paths_data = data_[\"embeddings\"], data_[\"paths\"]\n"
      ],
      "metadata": {
        "id": "_CoDLY4nP9ku"
      },
      "id": "_CoDLY4nP9ku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_data.shape"
      ],
      "metadata": {
        "id": "Jmye1ezDQALV"
      },
      "id": "Jmye1ezDQALV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nu.display_plots(\"kmeans_clustering_{counter:0>3}.svg\", directory=\"./ml\",\n",
        "                            text_top='kmeans_clustering_{counter:0>3}.tex', counter=(0, max_iteration))"
      ],
      "metadata": {
        "id": "XTu2xxrcQHTz"
      },
      "id": "XTu2xxrcQHTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eetSfLGLH6Jk"
      },
      "source": [
        "## Hierarchical Clustering\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/hierarchical-clustering.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/hierarchical-clustering.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Other approaches to clustering involve forming taxonomies of the cluster\n",
        "centers, like humans apply to animals, to form trees. Hierarchical\n",
        "clustering builds a tree structure showing the relationships between\n",
        "data points. We’ll demonstrate agglomerative clustering on the oil flow\n",
        "data set, which contains measurements from a multiphase flow facility."
      ],
      "id": "eetSfLGLH6Jk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPJGJYw2H6Jk"
      },
      "source": [
        "## Oil Flow Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/oil-flow-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/oil-flow-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "This data set is from a physics-based simulation of oil flow in a\n",
        "pipeline. The data was generated as part of a project to determine the\n",
        "fraction of oil, water and gas in North Sea oil pipes\n",
        "(**Bishop:gtm96?**)."
      ],
      "id": "gPJGJYw2H6Jk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR631FyoH6Jl"
      },
      "outputs": [],
      "source": [
        "import pods"
      ],
      "id": "WR631FyoH6Jl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuaa9AeFH6Jl"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.oil()"
      ],
      "id": "nuaa9AeFH6Jl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih7J0SFYH6Jm"
      },
      "source": [
        "The data consists of 1000 12-dimensional observations of simulated oil\n",
        "flow in a pipeline. Each observation is labelled according to the\n",
        "multi-phase flow configuration (homogeneous, annular or laminar)."
      ],
      "id": "ih7J0SFYH6Jm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqChIQe_H6Jn"
      },
      "outputs": [],
      "source": [
        "# Convert data[\"Y\"] from [1, -1, -1] in each row to rows of 0 or 1 or 2\n",
        "Y = data[\"Y\"]\n",
        "# Find rows with 1 in first column (class 0)\n",
        "class0 = (Y[:, 0] == 1).astype(int) * 0\n",
        "# Find rows with 1 in second column (class 1)\n",
        "class1 = (Y[:, 1] == 1).astype(int) * 1\n",
        "# Find rows with 1 in third column (class 2)\n",
        "class2 = (Y[:, 2] == 1).astype(int) * 2\n",
        "# Combine into single array of class labels 0,1,2\n",
        "labels = class0 + class1 + class2"
      ],
      "id": "OqChIQe_H6Jn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb9XP9HsH6Jn"
      },
      "source": [
        "The data is returned as a dictionary containing training and test inputs\n",
        "(‘X,’ ‘Xtst’), training and test labels (‘Y,’ ‘Ytst’), and the names of\n",
        "the features."
      ],
      "id": "nb9XP9HsH6Jn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyyxPK8JH6Jo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai\n",
        "import numpy as np"
      ],
      "id": "iyyxPK8JH6Jo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvvgWcq6H6Jp"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "# Plot first two dimensions of the data\n",
        "classes = np.unique(labels)\n",
        "colors = ['r', 'g', 'b']\n",
        "for i, cls in enumerate(classes):\n",
        "    idx = data['Y'] == cls\n",
        "    ax.plot(data['X'][idx, 0], data['X'][idx, 1], colors[i] + '.',\n",
        "            markersize=10, label=f'Class {cls}')\n",
        "ax.set_xlabel('1st dimension')\n",
        "ax.set_ylabel('2nd dimension')\n",
        "ax.legend()\n",
        "\n",
        "mlai.write_figure('oil-flow-data.svg', directory='./datasets')"
      ],
      "id": "WvvgWcq6H6Jp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SwTTrCRH6Jp"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//datasets/oil-flow-data.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Visualization of the first two dimensions of the oil flow\n",
        "data from Bishop and James (1993)</i>\n",
        "\n",
        "As normal we include the citation information for the data."
      ],
      "id": "9SwTTrCRH6Jp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wg3u-eIH6Jq"
      },
      "outputs": [],
      "source": [
        "print(data['citation'])"
      ],
      "id": "4wg3u-eIH6Jq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYYjv97hH6Jr"
      },
      "source": [
        "And extra information about the data is included, as standard, under the\n",
        "keys `info` and `details`."
      ],
      "id": "GYYjv97hH6Jr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Bxd_mhH6Js"
      },
      "outputs": [],
      "source": [
        "print(data['details'])"
      ],
      "id": "o4Bxd_mhH6Js"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0lAYOiDH6Js"
      },
      "outputs": [],
      "source": [
        "X = data['X']\n",
        "Y = data['Y']"
      ],
      "id": "-0lAYOiDH6Js"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoO0w21IH6KB"
      },
      "source": [
        "## Hierarchical Clustering of Oil Flow Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/oil-flow-hierarchical-clustering.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/oil-flow-hierarchical-clustering.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "In this example, we’ll apply hierarchical clustering to the oil flow\n",
        "data set. The data contains measurements from different flow regimes in\n",
        "a multiphase flow facility. The dendrogram shows how measurements\n",
        "naturally cluster into different flow types. Ward’s linkage method is\n",
        "used as it tends to create compact, evenly-sized clusters."
      ],
      "id": "YoO0w21IH6KB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI1EUxDGH6KC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import pods"
      ],
      "id": "LI1EUxDGH6KC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1YaI3FDH6KD"
      },
      "outputs": [],
      "source": [
        "# Perform hierarchical clustering\n",
        "linked = linkage(X, 'ward')  # Ward's method for minimum variance"
      ],
      "id": "H1YaI3FDH6KD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWmwjRKGH6KE"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from mlai import plot"
      ],
      "id": "gWmwjRKGH6KE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqltl8v6H6KE"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
        "dendrogram(linked,\n",
        "           orientation='top',\n",
        "           distance_sort='descending',\n",
        "           show_leaf_counts=True,\n",
        "           leaf_rotation=45,  # Rotate labels\n",
        "           leaf_font_size=8,  # Reduce font size\n",
        "           ax=ax)\n",
        "ax.set_xlabel('Sample Index')\n",
        "ax.set_ylabel('Distance')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels\n",
        "plt.tight_layout()\n",
        "mlai.write_figure('hierarchical-clustering-oil.svg', directory='./dimred')"
      ],
      "id": "Iqltl8v6H6KE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU8PsWi7H6KF"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/hierarchical-clustering-oil.svg\" class=\"\" width=\"60%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Hierarchical clustering applied to oil flow data. The\n",
        "dendrogram shows how different flow regimes are grouped based on their\n",
        "measurement similarities. The three main flow regimes (homogeneous,\n",
        "annular, and laminar) should form distinct clusters.</i>"
      ],
      "id": "oU8PsWi7H6KF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GMSZsHAH6KF"
      },
      "source": [
        "## Phylogenetic Trees\n",
        "\n",
        "A powerful application of hierarchical clustering is in constructing\n",
        "phylogenetic trees from genetic sequence data. By comparing DNA/RNA\n",
        "sequences across species, we can reconstruct their evolutionary\n",
        "relationships and estimate when species diverged from common ancestors.\n",
        "The resulting tree structure, called a phylogeny, maps out the\n",
        "evolutionary history and relationships between organisms.\n",
        "\n",
        "Modern phylogenetic methods go beyond simple clustering - they\n",
        "incorporate sophisticated models of genetic mutation and molecular\n",
        "evolution. These models can estimate not just the structure of\n",
        "relationships, but also the timing of evolutionary divergence events\n",
        "based on mutation rates. This has important applications in tracking the\n",
        "origins and spread of rapidly evolving pathogens like HIV and influenza\n",
        "viruses. Understanding viral phylogenies helps epidemiologists trace\n",
        "outbreak sources, track transmission patterns, and develop targeted\n",
        "containment strategies."
      ],
      "id": "6GMSZsHAH6KF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A38jtR8TH6KF"
      },
      "source": [
        "## Product Clustering\n",
        "\n",
        "An e-commerce company could apply hierarchical clustering to organize\n",
        "their product catalog into a taxonomy tree. Products would be grouped\n",
        "into increasingly specific categories - for example, Electronics might\n",
        "split into Phones, Computers, etc., with Phones further dividing into\n",
        "Smartphones, Feature Phones, and so on. This creates an intuitive\n",
        "hierarchical organization. However, many products naturally belong in\n",
        "multiple categories - for instance, running shoes could reasonably be\n",
        "classified as both sporting equipment and footwear. The strict tree\n",
        "structure of hierarchical clustering doesn’t allow for this kind of\n",
        "multiple categorization, which is a key limitation for product\n",
        "organization."
      ],
      "id": "A38jtR8TH6KF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0brWD9KH6KG"
      },
      "source": [
        "## Hierarchical Clustering Challenge\n",
        "\n",
        "Our psychological ability to form categories is far more sophisticated\n",
        "than hierarchical trees. Research in cognitive science has revealed that\n",
        "humans naturally form overlapping categories and learn abstract\n",
        "principles that guide classification. Josh Tenenbaum’s influential work\n",
        "demonstrates how human concept learning combines multiple forms of\n",
        "inference through hierarchical Bayesian models that integrate\n",
        "similarity-based clustering with theory-based reasoning. This\n",
        "computational approach aligns with foundational work by Eleanor Rosch on\n",
        "prototype theory and Susan Carey’s research on conceptual change,\n",
        "showing how categorization adapts to context and goals. While these\n",
        "cognitively-inspired models better capture human-like categorization,\n",
        "their computational complexity currently limits practical applications\n",
        "to smaller datasets. Nevertheless, they provide important insights into\n",
        "more flexible clustering approaches that could eventually enhance\n",
        "machine learning systems."
      ],
      "id": "k0brWD9KH6KG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88RI0aoqH6KG"
      },
      "source": [
        "## Other Clustering Approaches\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/other-clustering.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/other-clustering.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Spectral clustering (Shi and Malik (2000),Ng et al. (n.d.)) is a\n",
        "powerful technique that uses eigenvalues of similarity matrices to\n",
        "perform dimensionality reduction before clustering. Unlike k-means, it\n",
        "can identify clusters of arbitrary shape, making it effective for\n",
        "complex data like image segmentation or social networks.\n",
        "\n",
        "The Dirichlet process provides a Bayesian framework for clustering\n",
        "without pre-specifying the number of clusters. It’s particularly\n",
        "valuable in scenarios where new, previously unseen categories may emerge\n",
        "over time. For example, in species discovery, it can model the\n",
        "probability of finding new species while accounting for known ones. This\n",
        "“infinite clustering” property makes it well-suited for open-ended\n",
        "learning problems where the total number of categories is unknown."
      ],
      "id": "88RI0aoqH6KG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhqim3WoH6KH"
      },
      "source": [
        "## High Dimensional Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-data.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-data.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "To introduce high dimensional data, we will first of all introduce a\n",
        "hand written digit from the U.S. Postal Service handwritten digit data\n",
        "set (originally collected from scanning enveolopes) and used in the\n",
        "first convolutional neural network paper (Le Cun et al., 1989).\n",
        "\n",
        "Le Cun et al. (1989) downscaled the images to $16 \\times 16$, here we\n",
        "use an image at the original scale, containing 64 rows and 57 columns.\n",
        "Since the pixels are binary, and the number of dimensions is 3,648, this\n",
        "space contains $2^{3,648}$ possible images. So this space contains a lot\n",
        "more than just one digit."
      ],
      "id": "Xhqim3WoH6KH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKod1C7YH6KI"
      },
      "source": [
        "## USPS Samples\n",
        "\n",
        "If we sample from this space, taking each pixel independently from a\n",
        "probability which is given by the number of pixels which are ‘on’ in the\n",
        "original image, over the total number of pixels, we see images that look\n",
        "nothing like the original digit."
      ],
      "id": "rKod1C7YH6KI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTQtZHeiH6KI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import mlai\n",
        "import pods"
      ],
      "id": "sTQtZHeiH6KI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMLiZLPOH6KJ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "\n",
        "pods.access.download_url(\"https://github.com/lawrennd/slides/raw/gh-pages/diagrams/ml/br1561_6.3.pgm\",\n",
        "                         store_directory=\"./ml\")\n",
        "six_image = mlai.load_pgm(\"br1561_6.3.pgm\", directory =\"./ml\")\n",
        "rows = six_image.shape[0]\n",
        "col = six_image.shape[1]\n",
        "\n",
        "ax.imshow(six_image,interpolation='none').set_cmap('gray')\n",
        "mlai.write_figure(\"dem_six000.png\", directory=\"./dimred/\")\n",
        "for i in range(3):\n",
        "    rand_image = np.random.rand(rows, col)<((six_image>0).sum()/float(rows*col))\n",
        "    ax.imshow(rand_image,interpolation='none').set_cmap('gray')\n",
        "    mlai.write_figure('dem_six{i:0>3}.png'.format(i=i+1), directory=\"./dimred/\")"
      ],
      "id": "MMLiZLPOH6KJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJEl6dF-H6KL"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import IntSlider\n",
        "import notutils as nu"
      ],
      "id": "EJEl6dF-H6KL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "untXY_rJH6KM"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('dem_six{counter:0>3}.png', directory='./dimred', counter=IntSlider(0, 0, 3, 1))"
      ],
      "id": "untXY_rJH6KM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK1WBDG0H6KM"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_six000.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_six001.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_six002.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Figure: <i>Even if we sample every nano second from now until the end of\n",
        "the universe we won’t see the original six again.</i>\n",
        "\n",
        "Even if we sample every nanosecond from now until the end of the\n",
        "universe you won’t see the original six again."
      ],
      "id": "SK1WBDG0H6KM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nekaRqrWH6KN"
      },
      "source": [
        "## Simple Model of Digit\n",
        "\n",
        "So, an independent pixel model for this digit doesn’t seem sensible. The\n",
        "total space is enormous, and yet the space occupied by the type of data\n",
        "we’re interested in is relatively small.\n",
        "\n",
        "Consider a different type of model. One where we take a prototype six\n",
        "and we rotate it left and right to create new data."
      ],
      "id": "nekaRqrWH6KN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omk8YOEmH6KO"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-image"
      ],
      "id": "omk8YOEmH6KO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWedEOkfH6KW"
      },
      "outputs": [],
      "source": [
        "from skimage.transform import rotate"
      ],
      "id": "IWedEOkfH6KW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl23NcQPH6KX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
        "six_image = mlai.load_pgm('br1561_6.3.pgm', directory ='./ml')\n",
        "six_image = np.hstack([np.zeros((rows, 3)), six_image, np.zeros((rows, 4))])\n",
        "dim_one = np.asarray(six_image.shape)\n",
        "angles = range(360)\n",
        "i = 0\n",
        "Y = np.zeros((len(angles), np.prod(dim_one)))\n",
        "for angle in angles:\n",
        "    rot_image = rotate(six_image, angle)\n",
        "    dim_two = np.asarray(rot_image.shape)\n",
        "    start = [int(round((dim_two[0] - dim_one[0])/2)), int(round((dim_two[1] - dim_one[1])/2))]\n",
        "    crop_image = rot_image[start[0]+np.array(range(dim_one[0])), :][:, start[1]+np.array(range(dim_one[1]))]\n",
        "    Y[i, :] = crop_image.flatten()\n",
        "    ax.imshow(rot_image,interpolation='none').set_cmap('gray')\n",
        "    mlai.write_figure(f\"dem_six_rotate{angle:0>3}.png\", directory=\"./dimred/\")"
      ],
      "id": "Kl23NcQPH6KX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lqic2L3H6KZ"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "2lqic2L3H6KZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qf4df8CH6Ka"
      },
      "outputs": [],
      "source": [
        "nu.display_plots('dem_six_rotate{counter:0>3}.png', directory='./dimred', counter=(0, 6))"
      ],
      "id": "3qf4df8CH6Ka"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIWVBdDXH6Ka"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_six_rotate001.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_six_rotate003.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_six_rotate005.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Figure: <i>Rotate a prototype six to form a set of plausible sixes.</i>\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_manifold_print001.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "<td width=\"30%\">\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/dem_manifold_print002.png\" style=\"width:100%\">\n",
        "\n",
        "</td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Figure: <i>The rotated sixes projected onto the first two principal\n",
        "components of the ‘rotated data set.’ The data lives on a one\n",
        "dimensional manifold in the 3,648 dimensional space.</i>"
      ],
      "id": "fIWVBdDXH6Ka"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB93At5rH6Kc"
      },
      "source": [
        "## Low Dimensional Manifolds\n",
        "\n",
        "Of course, in practice pure rotation of the image is too simple a model.\n",
        "Digits can undergo several distortions and retain their nature. For\n",
        "example, they can be scaled, they can go through translation, they can\n",
        "udnergo ‘thinning.’ But, for data with ‘structure’ we expect fewer of\n",
        "these distortions than the dimension of the data. This means we expect\n",
        "data to live on a lower dimensonal manifold. This implies that we should\n",
        "deal with high dimensional data by looking for a lower dimensional\n",
        "(non-linear) embedding."
      ],
      "id": "GB93At5rH6Kc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_H_m-DtH6Kc"
      },
      "source": [
        "## High Dimensional Data Effects\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-effects.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-effects.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "In high dimensional spaces, our intuitions from everyday three\n",
        "dimensional space can fail dramatically. There are two major effects\n",
        "that occur:\n",
        "\n",
        "1.  All data moves to a “shell” at one standard deviation from the mean\n",
        "    (known as the “curse of dimensionality”)\n",
        "2.  Distances between points become constant\n",
        "\n",
        "Let’s explore these effects with some experiments."
      ],
      "id": "V_H_m-DtH6Kc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnSR-oUFH6Kf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mlai.plot as plot\n",
        "import matplotlib.pyplot as plt\n",
        "import mlai"
      ],
      "id": "GnSR-oUFH6Kf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS9rZx5HH6Kh"
      },
      "outputs": [],
      "source": [
        "# Generate high-dimensional Gaussian data\n",
        "d = 1000  # dimensions\n",
        "n = 100   # number of points\n",
        "Y = np.random.normal(size=(n, d))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "plot.distance_distribution(Y, title=f'Distance Distribution for {d}-D Gaussian Data', ax=ax)\n",
        "mlai.write_figure('high-d-distances.svg', directory='./dimred')"
      ],
      "id": "tS9rZx5HH6Kh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKDgAUk-H6Ki"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/high-d-distances.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Distribution of pairwise distances between points in\n",
        "high-dimensional Gaussian data. The red line shows the theoretical gamma\n",
        "distribution.</i>\n",
        "\n",
        "This plot shows the distribution of pairwise distances between points in\n",
        "our high-dimensional Gaussian data. The red line shows the theoretical\n",
        "prediction - a gamma distribution with shape parameter d/2 and scale\n",
        "2/d. The close match between theory and practice demonstrates how in\n",
        "high dimensions, distances between random points become highly\n",
        "concentrated around a particular value."
      ],
      "id": "eKDgAUk-H6Ki"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrXRJXL6H6Kj"
      },
      "source": [
        "## Structured High Dimensional Data\n",
        "\n",
        "Now let’s look at what happens when we introduce structure into our\n",
        "high-dimensional data. We’ll create data that actually lies in a\n",
        "lower-dimensional space, but embedded in high dimensions."
      ],
      "id": "CrXRJXL6H6Kj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKN6gzbzH6Kk"
      },
      "outputs": [],
      "source": [
        "# Generate data that lies on a 2D manifold embedded in 1000D\n",
        "W = np.random.normal(size=(d, 2))  # 2D latent directions\n",
        "latent = np.random.normal(size=(n, 2))  # 2D latent points\n",
        "Y_structured = latent @ W.T  # Project to high dimensions\n",
        "\n",
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "plot.distance_distribution(Y_structured,\n",
        "                         title='Distance Distribution for Structured High-D Data',\n",
        "                         ax=ax)\n",
        "mlai.write_figure('structured-high-d-distances.svg', directory='./dimred')"
      ],
      "id": "KKN6gzbzH6Kk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFv4dTMVH6Kl"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/structured-high-d-distances.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Distribution of pairwise distances for structured\n",
        "high-dimensional data. The distribution differs significantly from pure\n",
        "high-dimensional data, reflecting the underlying 2D structure.</i>\n",
        "\n",
        "Notice how the distance distribution for the structured data deviates\n",
        "significantly from what we would expect for truly high-dimensional data.\n",
        "Instead of matching the theoretical curve for 1000-dimensional data, it\n",
        "more closely resembles what we would expect for 2-dimensional data. This\n",
        "is because despite living in a 1000-dimensional space, the data actually\n",
        "has an intrinsic dimensionality of 2.\n",
        "\n",
        "This effect is exactly what we observe in real datasets - they typically\n",
        "have much lower intrinsic dimensionality than their ambient dimension\n",
        "would suggest. This is why dimensionality reduction techniques like PCA\n",
        "can be so effective: they help us discover and work with this\n",
        "lower-dimensional structure."
      ],
      "id": "bFv4dTMVH6Kl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW9ydj2jH6Km"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Generate your own high-dimensional dataset with known structure and\n",
        "visualize its distance distribution. Try varying the intrinsic\n",
        "dimensionality (e.g. use 3D or 4D latent space) and observe how the\n",
        "distance distribution changes. What happens if you add some noise to the\n",
        "structured data?"
      ],
      "id": "VW9ydj2jH6Km"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_izo1LmH6Kn"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 1 here"
      ],
      "id": "r_izo1LmH6Kn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBw8yKdbH6Ko"
      },
      "source": [
        "## High Dimensional Effects in Real Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-data-real.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/high-dimensional-data-real.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We’ve seen how high-dimensional random data behaves in a very specific\n",
        "way, with highly concentrated distance distributions. Now let’s examine\n",
        "some real datasets to see how they differ."
      ],
      "id": "iBw8yKdbH6Ko"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANSlcyAeH6Kp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mlai.plot as plot\n",
        "import matplotlib.pyplot as plt\n",
        "import pods"
      ],
      "id": "ANSlcyAeH6Kp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LW86SysH6Kq"
      },
      "source": [
        "First let’s look at a motion capture dataset, which despite having high\n",
        "dimension, is constrained by the physics of human movement."
      ],
      "id": "3LW86SysH6Kq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIDue6h4H6Kr"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.osu_run1()\n",
        "Y = data['Y']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "plot.distance_distribution(Y, title='Distance Distribution for Motion Capture Data', ax=ax)"
      ],
      "id": "ZIDue6h4H6Kr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBtvFv0JH6Kx"
      },
      "source": [
        "Notice how the distance distribution is much more spread out than we\n",
        "would expect for random high-dimensional data. This suggests that the\n",
        "motion capture data has much lower intrinsic dimensionality than its\n",
        "ambient dimension, due to the physical constraints of human movement."
      ],
      "id": "gBtvFv0JH6Kx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFJtQwzH6Ky"
      },
      "source": [
        "## Oil Flow Data\n",
        "\n",
        "Now let’s look at the oil flow dataset, which contains measurements from\n",
        "gamma densitometry sensors."
      ],
      "id": "mSFJtQwzH6Ky"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCmM7yj-H6Kz"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.oil()\n",
        "Y = data['X']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "plot.distance_distribution(Y, title='Distance Distribution for Oil Flow Data', ax=ax)"
      ],
      "id": "jCmM7yj-H6Kz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZWY9uakH6K0"
      },
      "source": [
        "Again we see a deviation from what we would expect for random\n",
        "high-dimensional data. The oil flow data shows clear structure in its\n",
        "distance distribution, reflecting the physical constraints of fluid\n",
        "dynamics that govern the system."
      ],
      "id": "YZWY9uakH6K0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4_z5XCFH6K0"
      },
      "source": [
        "## Implications for Dimensionality Reduction\n",
        "\n",
        "These examples demonstrate why dimensionality reduction can be so\n",
        "effective on real datasets:\n",
        "\n",
        "1.  Real data typically has strong constraints (physical, biological,\n",
        "    economic, etc.) that restrict it to a lower-dimensional manifold\n",
        "2.  This manifold structure is revealed by the distribution of pairwise\n",
        "    distances\n",
        "3.  Dimensionality reduction techniques like PCA can discover and\n",
        "    exploit this structure\n",
        "\n",
        "When we see a distance distribution that deviates from theoretical\n",
        "predictions for random high-dimensional data, it suggests that\n",
        "dimensionality reduction might be particularly effective."
      ],
      "id": "R4_z5XCFH6K0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-aaWE94H6K2"
      },
      "source": [
        "## Latent Variables and Dimensionality Reduction\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/latent-variable-motivation.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/latent-variable-motivation.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Why does dimensionality reduction work on real data? The key insight is\n",
        "that while our measurements may be very high-dimensional, the underlying\n",
        "phenomena we’re measuring often have much lower intrinsic\n",
        "dimensionality. For example:\n",
        "\n",
        "1.  A motion capture recording might have hundreds of coordinates, but\n",
        "    these are all generated by a person’s movements that have far fewer\n",
        "    degrees of freedom.\n",
        "\n",
        "2.  Genetic data may record thousands of gene expression levels, but\n",
        "    these are often controlled by a much smaller number of regulatory\n",
        "    processes.\n",
        "\n",
        "3.  Images contain millions of pixels, but the actual meaningful content\n",
        "    often lies on a much lower-dimensional manifold."
      ],
      "id": "g-aaWE94H6K2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-TztTHHH6K3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "_-TztTHHH6K3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLMr0OiuH6K4"
      },
      "outputs": [],
      "source": [
        "# Example showing how a 1D curve appears in 2D\n",
        "t = np.linspace(0, 2*np.pi, 100)\n",
        "x = np.column_stack([np.cos(t), np.sin(t)])\n",
        "\n",
        "# Plot the data\n",
        "fig, ax = plt.subplots(1, 2, figsize=plot.big_wide_figsize)\n",
        "ax[0].plot(x[:, 0], x[:, 1], 'b.')\n",
        "ax[0].set_xlabel('$x_1$')\n",
        "ax[0].set_ylabel('$x_2$')\n",
        "\n",
        "# Plot the latent variable representation\n",
        "ax[1].plot(t, np.zeros_like(t), 'r.')\n",
        "ax[1].set_xlabel('$z$')\n",
        "ax[1].set_yticks([])\n",
        "\n",
        "mlai.write_figure('latent-variable-example.svg', directory='./dimred')"
      ],
      "id": "ZLMr0OiuH6K4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RucB92ZYH6K6"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/latent-variable-example.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Data that appears 2-dimensional (left) can be described by a\n",
        "single latent variable $z$ (right) that traces out the curve.</i>\n",
        "\n",
        "This example shows how data that appears to be 2-dimensional (left) can\n",
        "actually be described by a single latent variable $z$ (right) that\n",
        "traces out the curve. The key premise of dimensionality reduction is\n",
        "finding and working with these simpler underlying representations."
      ],
      "id": "RucB92ZYH6K6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmKv4lFnH6K7"
      },
      "source": [
        "# Latent Variables\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/latent-variables.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/latent-variables.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Latent means hidden, and hidden variables are simply *unobservable*\n",
        "variables. The idea of a latent variable is crucial to the concept of\n",
        "artificial intelligence, machine learning and experimental design. A\n",
        "latent variable could take many forms. We might observe a man walking\n",
        "along a road with a large bag of clothes and we might *infer* that the\n",
        "man is walking to the laundrette. Our observations are a highly complex\n",
        "data space, the response in our eyes is processed through our visual\n",
        "cortex, the combination of the individual’s limb movememnts and the\n",
        "direction they are walking in all conflate in our heads to cause us to\n",
        "infer that (perhaps) the individual is going to the laundrette. We don’t\n",
        "*know* that the man is walking to the laundrette, but we have a model of\n",
        "the world that suggests that it’s a likely outcome for the very complex\n",
        "data. In some ways the latent variable can be seen as a *compression* of\n",
        "this very complex scene. If I were writing a book, I might write that “A\n",
        "man tripped over whilst walking to the laundrette.” In the reader’s mind\n",
        "an image of a man, perhaps laden with dirty clothes, may occur. All\n",
        "these ideas come from our expectations of the world around us. We can\n",
        "make further inference about the man, some of it perhaps plausible\n",
        "others less so. The man may be going to the laundrette because his\n",
        "washing machine is broken, or because he doesn’t have a large enough\n",
        "flat to have a washing machine, or because he’s carrying a duvet, or\n",
        "because he doesn’t like ironing. All of these may *increase* in\n",
        "probability given our observation, but they are still *latent*\n",
        "variables. Unless we follow the man back to his appartment, or start\n",
        "making other enquirires about the man, we don’t know the true answer.\n",
        "\n",
        "It’s clear that to do inference about any complex system we *must*\n",
        "include latent variables. Latent variables are extremely powerful. In\n",
        "robotics, they are used to represent the *state* of the robot. The state\n",
        "of the robot may include its position (in x, y coordinates) its speed,\n",
        "its direction of facing. How are *these* variables unknown to the robot?\n",
        "Well the robot only posesses *sensors*, it can make observations of the\n",
        "nearest object in a certain direction, and it may have a map of its\n",
        "environment. If we represent the state of the robot as its position on a\n",
        "map, it may be uncertain of that position. If you go walking or running\n",
        "in the hills around Sheffield, you can take a very high quality ordnance\n",
        "survey map with you. However, unless you are a really excellent\n",
        "orienteer, when you are far from any given landmark, you will probably\n",
        "be *uncertain* about your true position on the map. These states are\n",
        "also latent variables.\n",
        "\n",
        "In statistical analysis of experiments you try to control for each\n",
        "aspect of the experiment, in particular by *randomization*. So if I’m\n",
        "interested in the ability of a particular fertilizer to improve the\n",
        "yield of a particular plant I may design an experiment where I apply the\n",
        "fertilizer to some plants (the treatment group) and withold the\n",
        "fertilizer from others (the control group). I then test to see whether\n",
        "the yield from the treatment group is better (or worse) than the control\n",
        "group. I may find that I have an excellent yield for the treatment\n",
        "group. However, what if I’d (unknowlingly) planted all my treatment\n",
        "plants in a sunny part of the field, and all the control plants in a\n",
        "shady part of the field. That would also be a latent variable, in this\n",
        "case known as a *confounder*. In statistical experimental design\n",
        "*randomization* is used to attempt to eliminate the correlated effects\n",
        "of these confounders: you aim to ensure that if these confounders *do*\n",
        "exist their effects are not correlated with treatment and contorl. This\n",
        "is known as a [randomized control\n",
        "trial](http://en.wikipedia.org/wiki/Randomized_controlled_trial).\n",
        "\n",
        "Greek philosophers worried a great deal about what was knowable and what\n",
        "was unknowable. Adherents of [philosophical\n",
        "Skeptisism](http://en.wikipedia.org/wiki/Skepticism) were inspired by\n",
        "the idea that since your senses sometimes give you contradictory\n",
        "information, they cannot be trusted, and in extreme cases they chose to\n",
        "*ignore* their senses. This is an acknowledgement that very often the\n",
        "true state of the world cannot be known with precision. Unfortunately,\n",
        "these philosophers didn’t have a good understanding of probability, so\n",
        "they were unable to encapsulate their ideas through a *degree* of\n",
        "belief.\n",
        "\n",
        "We often use language to express the compression of a complex behavior\n",
        "or patterns in a simpler way, for example we talk about motives as a\n",
        "useful distallation for a perhaps very complex patter of behavior. In\n",
        "physics we use principles of causation and simple laws to describe the\n",
        "world around us. Such motives or underlying principles are difficult to\n",
        "observe directly, our conclusions about them emerge over a period of\n",
        "time by observing indirect consequences of the latent variables.\n",
        "\n",
        "Epistemic uncertainty allows us to deal with these worries by\n",
        "associating our degree of belief about the state of the world with a\n",
        "probaiblity distribution. This core idea underpins state space\n",
        "modelling, probabilistic graphical models and the wider field of latent\n",
        "variable modelling. In this session we are going to explore the idea in\n",
        "a simple linear system and see how it relates to *factor analysis* and\n",
        "*principal component analysis*."
      ],
      "id": "AmKv4lFnH6K7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P78syX_5H6K8"
      },
      "source": [
        "# Your Personality\n",
        "\n",
        "At the beginning of the 20th century there was a great deal of interest\n",
        "amoungst psychologists in formalizing patterns of thought. The approach\n",
        "they used became known as factor analysis. The principle is that we\n",
        "observe a potentially high dimensional vector of characteristics about\n",
        "an individual. To formalize this, social scientists designed\n",
        "questionaires. We can envisage many questions that we may ask, but the\n",
        "assumption is that underlying these questions there are only a few\n",
        "traits that dictate the behavior. These models are known as latent trait\n",
        "models and the analysis is sometimes known as factor analysis. The idea\n",
        "is that there are a few characteristic traits that we are looking to\n",
        "discern. These traits or factors can be extracted by assimilating the\n",
        "high dimensional characteristics of the individual into a few latent\n",
        "factors."
      ],
      "id": "P78syX_5H6K8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebn61fZJH6K9"
      },
      "source": [
        "## Factor Analysis Model\n",
        "\n",
        "This causes us to consider a model as follows, if we are given a high\n",
        "dimensional vector of features (perhaps questionaire answers) associated\n",
        "with an individual, $\\mathbf{ y}$, we assume that these factors are\n",
        "actually generated from a low dimensional vector latent traits, or\n",
        "latent variables, which determine the personality. $$\n",
        "\\mathbf{ y}= \\mathbf{f}(\\mathbf{ z}) + \\boldsymbol{ \\epsilon},\n",
        "$$ where $\\mathbf{f}(\\mathbf{ z})$ is a *vector valued* function that is\n",
        "dependent on the latent traits and $\\boldsymbol{ \\epsilon}$ is some\n",
        "corrupting noise. For simplicity, we assume that the function is given\n",
        "by a *linear* relationship, $$\n",
        "\\mathbf{f}(\\mathbf{ z}) = \\mathbf{W}\\mathbf{ z}\n",
        "$$ where we have introduced a matrix $\\mathbf{W}$ that is sometimes\n",
        "referred to as the *factor loadings* but we also immediately see is\n",
        "related to our *multivariate linear regression* models from the . That\n",
        "is because our vector valued function is of the form\n",
        "\n",
        "$$\n",
        "\\mathbf{f}(\\mathbf{ z}) =\n",
        "\\begin{bmatrix} f_1(\\mathbf{ z}) \\\\ f_2(\\mathbf{ z}) \\\\ \\vdots \\\\\n",
        "f_p(\\mathbf{ z})\\end{bmatrix}\n",
        "$$ where there are $p$ features associated with the individual. If we\n",
        "consider any of these functions individually we have a prediction\n",
        "function that looks like a regression model, $$\n",
        "f_j(\\mathbf{ z}) =\n",
        "\\mathbf{ w}_{j, :}^\\top \\mathbf{ z},\n",
        "$$ for each element of the vector valued function, where\n",
        "$\\mathbf{ w}_{:, j}$ is the $j$th column of the matrix $\\mathbf{W}$. In\n",
        "that context each column of $\\mathbf{W}$ is a vector of *regression\n",
        "weights*. This is a multiple input and multiple output regression. Our\n",
        "inputs (or covariates) have dimensionality greater than 1 and our\n",
        "outputs (or response variables) also have dimensionality greater than\n",
        "one. Just as in a standard regression, we are assuming that we don’t\n",
        "observe the function directly (note that this *also* makes the function\n",
        "a *type* of latent variable), but we observe some corrupted variant of\n",
        "the function, where the corruption is given by $\\boldsymbol{ \\epsilon}$.\n",
        "Just as in linear regression we can assume that this corruption is given\n",
        "by Gaussian noise, where the noise for the $j$th element of\n",
        "$\\mathbf{ y}$ is by, $$\n",
        "\\epsilon_j \\sim \\mathscr{N}\\left(0,\\sigma^2_j\\right).\n",
        "$$ Of course, just as in a regression problem we also need to make an\n",
        "assumption across the individual data points to form our full\n",
        "likelihood. Our data set now consists of many observations of\n",
        "$\\mathbf{ y}$ for diffetent individuals. We store these observations in\n",
        "a *design matrix*, $\\mathbf{Y}$, where each *row* of $\\mathbf{Y}$\n",
        "contains the observation for one individual. To emphasize that\n",
        "$\\mathbf{ y}$ is a vector derived from a row of $\\mathbf{Y}$ we\n",
        "represent the observation of the features associated with the $i$th\n",
        "individual by $\\mathbf{ y}_{i, :}$, and place each individual in our\n",
        "data matrix,\n",
        "\n",
        "$$\n",
        "\\mathbf{Y}\n",
        "= \\begin{bmatrix} \\mathbf{ y}_{1, :}^\\top \\\\ \\mathbf{ y}_{2, :}^\\top \\\\ \\vdots \\\\\n",
        "\\mathbf{ y}_{n, :}^\\top\\end{bmatrix},\n",
        "$$ where we have $n$ data points. Our data matrix therefore has $n$ rows\n",
        "and $p$ columns. The point to notice here is that each data obsesrvation\n",
        "appears as a row vector in the design matrix (thus the transpose\n",
        "operation inside the brackets). Our prediction functions are now\n",
        "actually a *matrix value* function, $$\n",
        "\\mathbf{F} = \\mathbf{Z}\\mathbf{W}^\\top,\n",
        "$$ where for each matrix the data points are in the rows and the data\n",
        "features are in the columns. This implies that if we have $q$ inputs to\n",
        "the function we have $\\mathbf{F}\\in \\Re^{n\\times p}$,\n",
        "$\\mathbf{W}\\in \\Re^{p \\times q}$ and $\\mathbf{Z}\\in \\Re^{n\\times q}$."
      ],
      "id": "ebn61fZJH6K9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY9xwN-OH6K9"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Show that, given all the definitions above, if, $$\n",
        "\\mathbf{F} = \\mathbf{Z}\\mathbf{W}^\\top\n",
        "$$ and the elements of the vector valued function $\\mathbf{F}$ are given\n",
        "by $$\n",
        "f_{i, j} = f_j(\\mathbf{ z}_{i, :}),\n",
        "$$ where $\\mathbf{ z}_{i, :}$ is the $i$th row of the latent variables,\n",
        "$\\mathbf{Z}$, then show that $$\n",
        "f_j(\\mathbf{ z}_{i, :}) = \\mathbf{ w}_{j, :}^\\top\n",
        "\\mathbf{ z}_{i, :}\n",
        "$$"
      ],
      "id": "IY9xwN-OH6K9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRy7SzSzH6K_"
      },
      "source": [
        "### Exercise 2 Answer\n",
        "\n",
        "Write your answer to Exercise 2 here"
      ],
      "id": "FRy7SzSzH6K_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qooNUHgyH6K_"
      },
      "source": [
        "## Latent Variables vs Linear Regression\n",
        "\n",
        "The difference between this model and a multiple output regression is\n",
        "that in the regression case we are provided with the covariates\n",
        "$\\mathbf{Z}$, here they are *latent variables*. These variables are\n",
        "unknown. Just as we have done in the past for unknowns, we now treat\n",
        "them with a probability distribution. In *factor analysis* we assume\n",
        "that the latent variables have a Gaussian density which is independent\n",
        "across both across the latent variables associated with the different\n",
        "data points, and across those associated with different data features,\n",
        "so we have, $$\n",
        "x_{i,j} \\sim\n",
        "\\mathscr{N}\\left(0,1\\right),\n",
        "$$ and we can write the density governing the latent variable associated\n",
        "with a single point as, $$\n",
        "\\mathbf{ z}_{i, :} \\sim \\mathscr{N}\\left(\\mathbf{0},\\mathbf{I}\\right).\n",
        "$$ If we consider the values of the function for the $i$th data point as\n",
        "$$\n",
        "\\mathbf{f}_{i, :} =\n",
        "\\mathbf{f}(\\mathbf{ z}_{i, :}) = \\mathbf{W}\\mathbf{ z}_{i, :}\n",
        "$$ then we can use the rules for multivariate Gaussian relationships to\n",
        "write that $$\n",
        "\\mathbf{f}_{i, :} \\sim \\mathscr{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top\\right)\n",
        "$$ which implies that the distribution for $\\mathbf{ y}_{i, :}$ is given\n",
        "by $$\n",
        "\\mathbf{ y}_{i, :} = \\sim \\mathscr{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\boldsymbol{\\Sigma}\\right)\n",
        "$$ where $\\boldsymbol{\\Sigma}$ the covariance of the noise variable,\n",
        "$\\epsilon_{i, :}$ which for factor analysis is a diagonal matrix\n",
        "(because we have assumed that the noise was *independent* across the\n",
        "features), $$\n",
        "\\boldsymbol{\\Sigma} = \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & 0\\\\\n",
        "0 & \\sigma^2_{2} & 0 & 0\\\\\n",
        "                                     0 & 0 & \\ddots &\n",
        "0\\\\\n",
        "                                     0 & 0 & 0 & \\sigma^2_p\\end{bmatrix}.\n",
        "$$ For completeness, we could also add in a *mean* for the data vector\n",
        "$\\boldsymbol{ \\mu}$, $$\n",
        "\\mathbf{ y}_{i, :} = \\mathbf{W}\\mathbf{ z}_{i, :} +\n",
        "\\boldsymbol{ \\mu}+ \\boldsymbol{ \\epsilon}_{i, :}\n",
        "$$ which would give our marginal distribution for $\\mathbf{ y}_{i, :}$ a\n",
        "mean $\\boldsymbol{ \\mu}$. However, the maximum likelihood solution for\n",
        "$\\boldsymbol{ \\mu}$ turns out to equal the empirical mean of the data,\n",
        "$$\n",
        "\\boldsymbol{ \\mu}= \\frac{1}{n} \\sum_{i=1}^n\n",
        "\\mathbf{ y}_{i, :},\n",
        "$$ *regardless* of the form of the covariance,\n",
        "$\\mathbf{C}= \\mathbf{W}\\mathbf{W}^\\top + \\boldsymbol{\\Sigma}$. As a\n",
        "result it is very common to simply preprocess the data and ensure it is\n",
        "zero mean. We will follow that convention for this session.\n",
        "\n",
        "The prior density over latent variables is independent, and the\n",
        "likelihood is independent, that means that the marginal likelihood here\n",
        "is also independent over the data points. Factor analysis was developed\n",
        "mainly in psychology and the social sciences for understanding\n",
        "personality and intelligence. [Charles\n",
        "Spearman](http://en.wikipedia.org/wiki/Charles_Spearman) was concerned\n",
        "with the measurements of “the abilities of man” and is credited with the\n",
        "earliest version of factor analysis."
      ],
      "id": "qooNUHgyH6K_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv77bG7aH6LA"
      },
      "source": [
        "# Principal Component Analysis\n",
        "\n",
        "In 1933 [Harold\n",
        "Hotelling](http://en.wikipedia.org/wiki/Harold_Hotelling) published on\n",
        "*principal component analysis* the first mention of this approach\n",
        "(Hotelling, 1933). Hotelling’s inspiration was to provide mathematical\n",
        "foundation for factor analysis methods that were by then widely used\n",
        "within psychology and the social sciences. His model was a factor\n",
        "analysis model, but he considered the noiseless ‘limit’ of the model. In\n",
        "other words he took $\\sigma^2_i \\rightarrow 0$ so that he had\n",
        "\n",
        "$$\n",
        "\\mathbf{ y}_{i, :} \\sim \\lim_{\\sigma^2 \\rightarrow 0} \\mathscr{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right).\n",
        "$$ The paper had two unfortunate effects. Firstly, the resulting model\n",
        "is no longer valid probablistically, because the covariance of this\n",
        "Gaussian is ‘degenerate.’ Because $\\mathbf{W}\\mathbf{W}^\\top$ has rank\n",
        "of at most $q$ where $q<p$ (due to the dimensionality reduction) the\n",
        "determinant of the covariance is zero, meaning the inverse doesn’t exist\n",
        "so the density, $$\n",
        "p(\\mathbf{ y}_{i, :}|\\mathbf{W}) =\n",
        "\\lim_{\\sigma^2 \\rightarrow 0} \\frac{1}{(2\\pi)^\\frac{p}{2}\n",
        "|\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}|^{\\frac{1}{2}}}\n",
        "\\exp\\left(-\\frac{1}{2}\\mathbf{ y}_{i, :}\\left[\\mathbf{W}\\mathbf{W}^\\top+ \\sigma^2\n",
        "\\mathbf{I}\\right]^{-1}\\mathbf{ y}_{i, :}\\right),\n",
        "$$ is *not* valid for $q<p$ (where $\\mathbf{W}\\in \\Re^{p\\times q}$).\n",
        "This mathematical consequence is a probability density which has no\n",
        "‘support’ in large regions of the space for $\\mathbf{ y}_{i, :}$. There\n",
        "are regions for which the probability of $\\mathbf{ y}_{i, :}$ is zero.\n",
        "These are any regions that lie off the hyperplane defined by mapping\n",
        "from $\\mathbf{ z}$ to $\\mathbf{ y}$ with the matrix $\\mathbf{W}$. In\n",
        "factor analysis the noise corruption, $\\boldsymbol{ \\epsilon}$, allows\n",
        "for points to be found away from the hyperplane. In Hotelling’s PCA the\n",
        "noise variance is zero, so there is only support for points that fall\n",
        "precisely on the hyperplane. Secondly, Hotelling explicity chose to\n",
        "rename factor analysis as principal component analysis, arguing that the\n",
        "factors social scientist sought were different in nature to the concept\n",
        "of a mathematical factor. This was unfortunate because the factor\n",
        "loadings, $\\mathbf{W}$ can also be seen as factors in the mathematical\n",
        "sense because the model Hotelling defined is a Gaussian model with\n",
        "covariance given by $\\mathbf{C}= \\mathbf{W}\\mathbf{W}^\\top$ so\n",
        "$\\mathbf{W}$ is a *factor* of the covariance in the mathematical sense,\n",
        "as well as a factor loading.\n",
        "\n",
        "However, the paper had one great advantage over standard approaches to\n",
        "factor analysis. Despite the fact that the model was a special case that\n",
        "is subsumed by the more general approach of factor analysis it is this\n",
        "special case that leads to a particular algorithm, namely that the\n",
        "factor loadings (or principal components as Hotelling referred to them)\n",
        "are given by an *eigenvalue decomposition* of the empirical covariance\n",
        "matrix."
      ],
      "id": "Rv77bG7aH6LA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byld70gVH6LB"
      },
      "source": [
        "## Computation of the Marginal Likelihood\n",
        "\n",
        "$$\n",
        "\\mathbf{ y}_{i,:}=\\mathbf{W}\\mathbf{ z}_{i,:}+\\boldsymbol{ \\epsilon}_{i,:},\\quad \\mathbf{ z}_{i,:} \\sim \\mathscr{N}\\left(\\mathbf{0},\\mathbf{I}\\right), \\quad \\boldsymbol{ \\epsilon}_{i,:} \\sim \\mathscr{N}\\left(\\mathbf{0},\\sigma^{2}\\mathbf{I}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{W}\\mathbf{ z}_{i,:} \\sim \\mathscr{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{W}\\mathbf{ z}_{i, :} + \\boldsymbol{ \\epsilon}_{i, :} \\sim \\mathscr{N}\\left(\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
        "$$\n",
        "\n",
        "**Probabilistic PCA Max. Likelihood Soln** (Tipping and Bishop (1999a))\n",
        "\n",
        "<img class=\"\" src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/ppca_graph.png\" style=\"width:40%\">\n",
        "\n",
        "Figure: <i>Graphical model representing probabilistic PCA.</i>\n",
        "\n",
        "$$p\\left(\\mathbf{Y}|\\mathbf{W}\\right)=\\prod_{i=1}^{n}\\mathscr{N}\\left(\\mathbf{ y}_{i, :}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)$$"
      ],
      "id": "byld70gVH6LB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CI00n3tH6LC"
      },
      "source": [
        "## Eigenvalue Decomposition\n",
        "\n",
        "Eigenvalue problems are widespreads in physics and mathematics, they are\n",
        "often written as a matrix/vector equation but we prefer to write them as\n",
        "a full matrix equation. In an eigenvalue problem you are looking to find\n",
        "a matrix of eigenvectors, $\\mathbf{U}$ and a *diagonal* matrix of\n",
        "eigenvalues, $\\boldsymbol{\\Lambda}$ that satisfy the *matrix* equation\n",
        "$$\n",
        "\\mathbf{A}\\mathbf{U} = \\mathbf{U}\\boldsymbol{\\Lambda}.\n",
        "$$ where $\\mathbf{A}$ is your matrix of interest. This equation is not\n",
        "trivially solvable through matrix inverse because matrix multiplication\n",
        "is not [commutative](http://en.wikipedia.org/wiki/Commutative_property),\n",
        "so premultiplying by $\\mathbf{U}^{-1}$ gives $$\n",
        "\\mathbf{U}^{-1}\\mathbf{A}\\mathbf{U}\n",
        "= \\boldsymbol{\\Lambda},\n",
        "$$ where we remember that $\\boldsymbol{\\Lambda}$ is a *diagonal* matrix,\n",
        "so the eigenvectors can be used to *diagonalise* the matrix. When\n",
        "performing the eigendecomposition on a Gaussian covariances,\n",
        "diagonalisation is very important because it returns the covariance to a\n",
        "form where there is no correlation between points."
      ],
      "id": "6CI00n3tH6LC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBpUeJJ5H6LD"
      },
      "source": [
        "## Positive Definite\n",
        "\n",
        "We are interested in the case where $\\mathbf{A}$ is a covariance matrix,\n",
        "which implies it is *positive definite*. A positive definite matrix is\n",
        "one for which the inner product, $$\n",
        "\\mathbf{ w}^\\top \\mathbf{C}\\mathbf{ w}\n",
        "$$ is positive for *all* values of the vector $\\mathbf{ w}$ other than\n",
        "the zero vector. One way of creating a positive definite matrix is to\n",
        "assume that the symmetric and positive definite matrix\n",
        "$\\mathbf{C}\\in \\Re^{p\\times p}$ is factorised into,\n",
        "$\\mathbf{A}in \\Re^{p\\times p}$, a *full rank* matrix, so that $$\n",
        "\\mathbf{C}= \\mathbf{A}^\\top\n",
        "\\mathbf{A}.\n",
        "$$ This ensures that $\\mathbf{C}$ must be positive definite because $$\n",
        "\\mathbf{ w}^\\top \\mathbf{C}\\mathbf{ w}=\\mathbf{ w}^\\top\n",
        "\\mathbf{A}^\\top\\mathbf{A}\\mathbf{ w}\n",
        "$$ and if we now define a new *vector* $\\mathbf{b}$ as $$\n",
        "\\mathbf{b} = \\mathbf{A}\\mathbf{ w}\n",
        "$$ we can now rewrite as $$\n",
        "\\mathbf{ w}^\\top \\mathbf{C}\\mathbf{ w}= \\mathbf{b}^\\top\\mathbf{b} = \\sum_{i}\n",
        "b_i^2\n",
        "$$ which, since it is a sum of squares, is positive or zero. The\n",
        "constraint that $\\mathbf{A}$ must be *full rank* ensures that there is\n",
        "no vector $\\mathbf{ w}$, other than the zero vector, which causes the\n",
        "vector $\\mathbf{b}$ to be all zeros."
      ],
      "id": "YBpUeJJ5H6LD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcHROJkHH6LE"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "If $\\mathbf{C}=\\mathbf{A}^\\top \\mathbf{A}$ then express $c_{i,j}$, the\n",
        "value of the element at the $i$th row and the $j$th column of\n",
        "$\\mathbf{C}$, in terms of the columns of $\\mathbf{A}$. Use this to show\n",
        "that (i) the matrix is symmetric and (ii) the matrix has positive\n",
        "elements along its diagonal."
      ],
      "id": "dcHROJkHH6LE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0GcHJhUH6LF"
      },
      "source": [
        "### Exercise 3 Answer\n",
        "\n",
        "Write your answer to Exercise 3 here"
      ],
      "id": "c0GcHJhUH6LF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPyct37aH6LG"
      },
      "source": [
        "## Eigenvectors of a Symmetric Matric\n",
        "\n",
        "Symmetric matrices have *orthonormal* eigenvectors. This means that\n",
        "$\\mathbf{U}$ is an [orthogonal\n",
        "matrix](http://en.wikipedia.org/wiki/Orthogonal_matrix),\n",
        "$\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$. This implies that\n",
        "$\\mathbf{u}_{:, i} ^\\top \\mathbf{u}_{:, j}$ is equal to 0 if $i\\neq j$\n",
        "and 1 if $i=j$."
      ],
      "id": "KPyct37aH6LG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjCWbEGCH6LN"
      },
      "source": [
        "## Practical Considerations\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/practical-dimensionality-reduction.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/practical-dimensionality-reduction.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "When applying dimensionality reduction in practice, there are several\n",
        "important considerations:\n",
        "\n",
        "1.  How to choose the latent dimensionality\n",
        "2.  How to validate the reduction is capturing important structure\n",
        "3.  What preprocessing steps are needed"
      ],
      "id": "LjCWbEGCH6LN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-PygYqeH6LN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot"
      ],
      "id": "J-PygYqeH6LN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgGWTMQQH6LO"
      },
      "outputs": [],
      "source": [
        "def plot_data_reconstruction(X, dim_reducer, num_components):\n",
        "    \"\"\"Visualize reconstruction error vs number of components\"\"\"\n",
        "    errors = []\n",
        "    dims = range(1, num_components+1)\n",
        "\n",
        "    for d in dims:\n",
        "        dim_reducer.set_params(n_components=d)\n",
        "        X_reduced = dim_reducer.fit_transform(X)\n",
        "        X_reconstructed = dim_reducer.inverse_transform(X_reduced)\n",
        "        error = np.mean((X - X_reconstructed) ** 2)\n",
        "        errors.append(error)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=plot.big_wide_figsize)\n",
        "    ax.plot(dims, errors, 'b-')\n",
        "    ax.set_xlabel('Number of Components')\n",
        "    ax.set_ylabel('Reconstruction Error')\n",
        "    ax.set_yscale('log')"
      ],
      "id": "rgGWTMQQH6LO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVRksZPGH6LP"
      },
      "source": [
        "One common approach for choosing the latent dimensionality is to look at\n",
        "the reconstruction error as a function of the number of components. An\n",
        "“elbow” in this curve often suggests a good tradeoff between complexity\n",
        "and accuracy.\n",
        "\n",
        "Before applying dimensionality reduction, it’s usually important to: 1.\n",
        "Center the data by removing the mean 2. Scale features to have unit\n",
        "variance if they’re on different scales 3. Handle missing values\n",
        "appropriately"
      ],
      "id": "JVRksZPGH6LP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnrGcNDFH6LR"
      },
      "source": [
        "## When Dimensionality Reduction Fails\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/dimensionality-reduction-failure-modes.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/dimensionality-reduction-failure-modes.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "While dimensionality reduction is powerful, it’s important to understand\n",
        "when it can fail:\n",
        "\n",
        "1.  When the data really is high dimensional with no simpler structure\n",
        "2.  When the relationship between dimensions is highly nonlinear\n",
        "3.  When different parts of the data have different intrinsic\n",
        "    dimensionality"
      ],
      "id": "dnrGcNDFH6LR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGYHTiMJH6LS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot\n",
        "import mlai"
      ],
      "id": "kGYHTiMJH6LS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtQR3pwrH6LT"
      },
      "source": [
        "## The Swiss Roll Example"
      ],
      "id": "UtQR3pwrH6LT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUKRYlhxH6LU"
      },
      "outputs": [],
      "source": [
        "# Generate data that lies on a Swiss roll manifold\n",
        "def generate_swiss_roll(n_points=1000):\n",
        "    t = 1.5 * np.pi * (1 + 2 * np.random.rand(n_points))\n",
        "    y = 21 * np.random.rand(n_points)\n",
        "    x = t * np.cos(t)\n",
        "    z = t * np.sin(t)\n",
        "    return np.column_stack((x, y, z)), t\n",
        "\n",
        "X, t = generate_swiss_roll()\n",
        "\n",
        "fig = plt.figure(figsize=plot.big_wide_figsize)\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap='viridis')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$y$')\n",
        "ax.set_zlabel('$z$')\n",
        "plt.colorbar(scatter)\n",
        "\n",
        "mlai.write_figure('swiss-roll.svg', directory='./dimred')"
      ],
      "id": "aUKRYlhxH6LU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYCubiK2H6LU"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/swiss-roll.svg\" class=\"\" width=\"80%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>The Swiss roll dataset is a classic example of data with\n",
        "nonlinear structure. The color represents the position along the roll,\n",
        "showing how points that are far apart in the original space are actually\n",
        "close in the intrinsic manifold.</i>"
      ],
      "id": "bYCubiK2H6LU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kk3uET2H6LV"
      },
      "source": [
        "## Common Failure Modes\n",
        "\n",
        "This example shows data lying on a Swiss roll manifold. Linear\n",
        "dimensionality reduction methods like PCA will fail to capture the\n",
        "structure of this data, while nonlinear methods like t-SNE or UMAP may\n",
        "perform better.\n",
        "\n",
        "Common failure modes include: 1. Using linear methods on nonlinear\n",
        "manifolds 2. Assuming global structure when only local structure exists\n",
        "3. Not accounting for noise in the data"
      ],
      "id": "4kk3uET2H6LV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joul8NbNH6LV"
      },
      "source": [
        "## Principal Component Analysis\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/principal-component-analysis.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/principal-component-analysis.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Principal Component Analysis (PCA) is one of the most fundamental and\n",
        "widely used dimensionality reduction techniques. While commonly credited\n",
        "to Pearson (1901), who was interested in finding “lines and planes of\n",
        "closest fit to systems of points in space,” the method as we will review\n",
        "it today was introduced and *named* by Hotelling (Hotelling (1933)). The\n",
        "approach is often used as a way of looking for correlations, but\n",
        "Hotelling’s motivation was a principled alternative to Spearman’s factor\n",
        "analysis (Spearman, 1904).\n",
        "\n",
        "Often PCA is introduced as a method for finding directions of maximum\n",
        "variance in high-dimensional data, and this is the interpretation that\n",
        "is due to Pearson (1901), but philosophically these approaches arre\n",
        "different even though they turn out to be identical mathematically. In a\n",
        "very real sense “many models lead to the PCA algorithm.” But these\n",
        "equivalences are only true when a *linear* interpretation is sought.\n",
        "Nonlinear extensions of these ideas (maximum variance directions,\n",
        "eigenvalue problems, latent variable models) all lead to *different*\n",
        "algorithms. However, since the linear algorithm has so many\n",
        "interpretations it is a wise place to begin analysis.\n",
        "\n",
        "The mathematical foundation of PCA relies on analyzing the sample\n",
        "covariance matrix of the data. For a dataset with $n$ points, this\n",
        "matrix is given by:\n",
        "\n",
        "$$\n",
        "\\mathbf{S}=\\frac{1}{n}\\sum_{i=1}^n\\left(\\mathbf{ y}_{i, :}-\\boldsymbol{ \\mu}\\right)\\left(\\mathbf{ y}_{i, :} - \\boldsymbol{ \\mu}\\right)^\\top\n",
        "$$\n",
        "\n",
        "The modern interpretation focuses on finding a set of orthogonal\n",
        "directions (principal components) along which the data varies the most,\n",
        "but Hotelling’s original formulation was derived from the idea of latent\n",
        "variables. The optimization problem we solve today, which maximizes the\n",
        "variance along each direction while maintaining orthogonality\n",
        "constraints, arises as the solution of the original latent variable\n",
        "formulation (Tipping and Bishop, 1999a)."
      ],
      "id": "joul8NbNH6LV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6e7DFqAH6LW"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "R6e7DFqAH6LW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Cg32ByAH6LW"
      },
      "outputs": [],
      "source": [
        "# Generate correlated 2D Gaussian samples\n",
        "num_points = 200\n",
        "mean = np.array([0, 0])\n",
        "cov = np.array([[2.0, 1.8],\n",
        "                [1.8, 2.0]])\n",
        "X = np.random.multivariate_normal(mean, cov, num_points)"
      ],
      "id": "-Cg32ByAH6LW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8yrTR7lH6LX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai.plot as plot"
      ],
      "id": "J8yrTR7lH6LX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bQ-ApmmH6LY"
      },
      "outputs": [],
      "source": [
        "def pca_plot(ax, X):\n",
        "    # Compute eigenvalues and eigenvectors of covariance matrix\n",
        "    eigvals, eigvecs = np.linalg.eigh(np.cov(X.T))\n",
        "\n",
        "    # Plot data points\n",
        "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
        "    ax.set_xlabel('$x_1$')\n",
        "    ax.set_ylabel('$x_2$')\n",
        "    ax.axis('equal')\n",
        "\n",
        "    mlai.write_figure(\"pca-directions-000.svg\", directory='./dimred')\n",
        "\n",
        "    # Plot covariance ellipse\n",
        "    # Get eigenvalues and eigenvectors of covariance matrix\n",
        "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
        "\n",
        "    # Calculate angle of rotation from largest eigenvector\n",
        "    angle = np.arctan2(eigenvecs[1,1], eigenvecs[0,1])\n",
        "\n",
        "    # Calculate width and height of ellipse from eigenvalues\n",
        "    width = 2 * np.sqrt(eigenvals[1])  # 2 std deviations\n",
        "    height = 2 * np.sqrt(eigenvals[0])\n",
        "\n",
        "    # Create and add ellipse patch\n",
        "    ellipse = plt.matplotlib.patches.Ellipse(mean, width, height,\n",
        "                                           angle=angle * 180/np.pi,\n",
        "                                           facecolor='none',\n",
        "                                           edgecolor='r',\n",
        "                                           linestyle='--')\n",
        "    ax.add_patch(ellipse)\n",
        "    mlai.write_figure(\"pca-directions-001.svg\", directory='./dimred')\n",
        "\n",
        "    # Plot principal components\n",
        "    counter = 1\n",
        "    for i in [0, 1]:\n",
        "        counter += 1\n",
        "        ax.arrow(mean[0], mean[1],\n",
        "                eigvecs[0, i]*np.sqrt(eigvals[i]),\n",
        "                eigvecs[1, i]*np.sqrt(eigvals[i]),\n",
        "                head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
        "        mlai.write_figure(f\"pca-directions-{counter:03d}.svg\", directory='./dimred')"
      ],
      "id": "9bQ-ApmmH6LY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxxM7HpsH6La"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_figsize())\n",
        "pca_plot(ax, X)"
      ],
      "id": "xxxM7HpsH6La"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-rfg2cAH6Lc"
      },
      "outputs": [],
      "source": [
        "import notutils as nu"
      ],
      "id": "J-rfg2cAH6Lc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hkWinJEH6Ld"
      },
      "outputs": [],
      "source": [
        "nu.display_plots(\"pca-directions-{counter:0>3}.svg\", directory=\"./dimred\", counter=(0, 3))"
      ],
      "id": "9hkWinJEH6Ld"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8emMO0FH6Lp"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/pca-directions-003.svg\" class=\"\" width=\"\\width\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Illustration of PCA on 2D correlated Gaussian data. The\n",
        "arrows show the principal components (eigenvectors scaled by square root\n",
        "of eigenvalues). The ellipse represents the covariance structure.</i>"
      ],
      "id": "n8emMO0FH6Lp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWcD8VQYH6Lq"
      },
      "source": [
        "## Probabilistic PCA\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/probabilistic-pca.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/probabilistic-pca.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "This linear relationship between the observed data and the latent\n",
        "variables is at the heart of Hotelling’s original formulation of PCA. It\n",
        "explicitly models the idea that the observed high-dimensional data is\n",
        "generated from a lower-dimensional set of latent variables, with some\n",
        "added noise. This perspective aligns PCA more closely with factor\n",
        "analysis and highlights its nature as a latent variable model.\n",
        "\n",
        "Probabilistic PCA (PPCA) provides a probabilistic interpretation of PCA\n",
        "by explicitly modeling the generative process that creates the observed\n",
        "data. The model assumes that each high-dimensional data point\n",
        "$\\mathbf{ y}_{i,:}$ is generated from a lower-dimensional latent\n",
        "variable $\\mathbf{ z}_{i,:}$ through a linear transformation with added\n",
        "Gaussian noise: $$\n",
        "\\mathbf{ y}_{i,:} = \\mathbf{W}\\mathbf{ z}_{i,:} + \\boldsymbol{ \\epsilon}_{i,:}\n",
        "$$ where\n",
        "$\\boldsymbol{ \\epsilon}_{i,:} \\sim \\mathscr{N}\\left(\\mathbf{0},\\sigma^2\\mathbf{I}\\right)$\n",
        "represents isotropic Gaussian noise. The model places a standard\n",
        "Gaussian prior over the latent variables: $$\n",
        "p(\\mathbf{ z}_{i,:}) = \\mathscr{N}\\left(\\mathbf{ z}_{i,:}|\\mathbf{0},\\mathbf{I}\\right)\n",
        "$$ Given these assumptions, the conditional probability of observing a\n",
        "data point given its latent representation is: $$\n",
        "p(\\mathbf{ y}_{i,:}|\\mathbf{ z}_{i,:},\\mathbf{W}) = \\mathscr{N}\\left(\\mathbf{ y}_{i,:}|\\mathbf{W}\\mathbf{ z}_{i,:},\\sigma^2\\mathbf{I}\\right)\n",
        "$$ By integrating out the latent variables, we obtain the marginal\n",
        "likelihood of the data: $$\n",
        "p(\\mathbf{ y}_{i,:}|\\mathbf{W}) = \\mathscr{N}\\left(\\mathbf{ y}_{i,:}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^{\\top}+\\sigma^{2}\\mathbf{I}\\right)\n",
        "$$ This probabilistic formulation, developed by Tipping and Bishop\n",
        "(Tipping and Bishop, 1999a), provides a principled framework that not\n",
        "only recovers classical PCA as a special case when $\\sigma^2 \\to 0$, but\n",
        "also enables extensions like handling missing data and mixture models."
      ],
      "id": "gWcD8VQYH6Lq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0it8J2JH6Ls"
      },
      "outputs": [],
      "source": [
        "from mlai import plot\n",
        "import mlai\n",
        "from matplotlib import pyplot as plt"
      ],
      "id": "D0it8J2JH6Ls"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzUa7SzWH6Lv"
      },
      "outputs": [],
      "source": [
        "pgm = plot.ppca_graph()\n",
        "filename = mlai.filename_join('ppca_graph.svg', directory='./dimred')\n",
        "pgm.render().figure.savefig(filename, transparent=True)"
      ],
      "id": "AzUa7SzWH6Lv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKx3Hjn1H6L1"
      },
      "source": [
        "<img src=\"https://mlatcl.github.io/mlfc/./slides/diagrams//dimred/ppca_graph.svg\" class=\"\" width=\"40%\" style=\"vertical-align:middle;\">\n",
        "\n",
        "Figure: <i>Graphical model representing probabilistic PCA.</i>"
      ],
      "id": "oKx3Hjn1H6L1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-WUQUmNH6L2"
      },
      "source": [
        "# Probabilistic PCA\n",
        "\n",
        "In 1997 [Tipping and\n",
        "Bishop](http://research.microsoft.com/pubs/67218/bishop-ppca-jrss.pdf)\n",
        "(Tipping and Bishop, 1999b) and\n",
        "[Roweis](https://www.cs.nyu.edu/~roweis/papers/empca.pdf) (Roweis, n.d.)\n",
        "independently revisited Hotelling’s model and considered the case where\n",
        "the noise variance was finite, but *shared* across all output dimensons.\n",
        "Their model can be thought of as a factor analysis where $$\n",
        "\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}.\n",
        "$$ This leads to a marginal likelihood of the form $$\n",
        "p(\\mathbf{Y}|\\mathbf{W}, \\sigma^2)\n",
        "= \\prod_{i=1}^n\\mathscr{N}\\left(\\mathbf{ y}_{i, :}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
        "$$ where the limit of $\\sigma^2\\rightarrow 0$ is *not* taken. This\n",
        "defines a proper probabilistic model. Tippping and Bishop then went on\n",
        "to prove that the *maximum likelihood* solution of this model with\n",
        "respect to $\\mathbf{W}$ is given by an eigenvalue problem. In the\n",
        "probabilistic PCA case the eigenvalues and eigenvectors are given as\n",
        "follows. $$\n",
        "\\mathbf{W}= \\mathbf{U}\\mathbf{L} \\mathbf{R}^\\top\n",
        "$$ where $\\mathbf{U}$ is the eigenvectors of the empirical covariance\n",
        "matrix $$\n",
        "\\mathbf{S} = \\sum_{i=1}^n(\\mathbf{ y}_{i, :} - \\boldsymbol{ \\mu})(\\mathbf{ y}_{i,:} - \\boldsymbol{ \\mu})^\\top,\n",
        "$$ which can be written\n",
        "$\\mathbf{S} = \\frac{1}{n} \\mathbf{Y}^\\top\\mathbf{Y}$ if the data is zero\n",
        "mean. The matrix $\\mathbf{L}$ is diagonal and is dependent on the\n",
        "*eigenvalues* of $\\mathbf{S}$, $\\boldsymbol{\\Lambda}$. If the $i$th\n",
        "diagonal element of this matrix is given by $\\lambda_i$ then the\n",
        "corresponding element of $\\mathbf{L}$ is $$\n",
        "\\ell_i = \\sqrt{\\lambda_i - \\sigma^2}\n",
        "$$ where $\\sigma^2$ is the noise variance. Note that if $\\sigma^2$ is\n",
        "larger than any particular eigenvalue, then that eigenvalue (along with\n",
        "its corresponding eigenvector) is *discarded* from the solution."
      ],
      "id": "X-WUQUmNH6L2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j821TgEvH6MI"
      },
      "source": [
        "## PPCA as Manifold Learning\n",
        "\n",
        "Probabilistic PCA can be viewed as a simple manifold learning algorithm.\n",
        "It assumes that:\n",
        "\n",
        "1.  The data lies near a linear manifold (subspace) of the\n",
        "    high-dimensional space\n",
        "2.  The deviation from this manifold is Gaussian noise\n",
        "3.  The intrinsic dimensionality is specified by the number of retained\n",
        "    components\n",
        "\n",
        "This view helps explain why PPCA works well when the manifold hypothesis\n",
        "holds and the manifold is approximately linear. When the manifold is\n",
        "nonlinear, we need more sophisticated methods like kernel PCA or neural\n",
        "network-based approaches."
      ],
      "id": "j821TgEvH6MI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmn5zU3aH6MJ"
      },
      "source": [
        "## Python Implementation of Probabilistic PCA\n",
        "\n",
        "We will now implement this algorithm in python."
      ],
      "id": "Hmn5zU3aH6MJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gehzuStQH6MK"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ],
      "id": "gehzuStQH6MK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICaztzYuH6ML"
      },
      "outputs": [],
      "source": [
        "# probabilistic PCA algorithm\n",
        "def ppca(Y, q):\n",
        "    # remove mean\n",
        "    Y_cent = Y - Y.mean(0)\n",
        "\n",
        "    # Comute covariance\n",
        "    S = np.dot(Y_cent.T, Y_cent)/Y.shape[0]\n",
        "    lambd, U = np.linalg.eig(S)\n",
        "\n",
        "    # Choose number of eigenvectors\n",
        "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
        "    l = np.sqrt(lambd[:q]-sigma2)\n",
        "    W = U[:, :q]*l[None, :]\n",
        "    return W, sigma2"
      ],
      "id": "ICaztzYuH6ML"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIMmiytKH6MM"
      },
      "source": [
        "In practice we may not wish to compute the eigenvectors of the\n",
        "covariance matrix directly. This is because it requires us to estimate\n",
        "the covariance, which involves a sum of squares term, before estimating\n",
        "the eigenvectors. We can estimate the eigenvectors directly either\n",
        "through [QR\n",
        "decomposition](http://en.wikipedia.org/wiki/QR_decomposition) or\n",
        "[singular value\n",
        "decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
        "We saw a similar issue arise when , where we also wished to avoid\n",
        "computation of $\\mathbf{Z}^\\top\\mathbf{Z}$ (or in the case of\n",
        "$\\boldsymbol{\\Phi}^\\top\\boldsymbol{\\Phi}$)."
      ],
      "id": "OIMmiytKH6MM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGXTQqWcH6MN"
      },
      "source": [
        "## Posterior for Principal Component Analysis\n",
        "\n",
        "Under the latent variable model justification for principal component\n",
        "analysis, we are normally interested in inferring something about the\n",
        "latent variables given the data. This is the distribution, $$\n",
        "p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :})\n",
        "$$ for any given data point. Determining this density turns out to be\n",
        "very similar to the approach for determining the Bayesian posterior of\n",
        "$\\mathbf{ w}$ in Bayesian linear regression, only this time we place the\n",
        "prior density over $\\mathbf{ z}_{i, :}$ instead of $\\mathbf{ w}$. The\n",
        "posterior is proportional to the joint density as follows, $$\n",
        "p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) \\propto p(\\mathbf{ y}_{i,\n",
        ":}|\\mathbf{W}, \\mathbf{ z}_{i, :}, \\sigma^2) p(\\mathbf{ z}_{i, :})\n",
        "$$ And as in the Bayesian linear regression case we first consider the\n",
        "log posterior, $$\n",
        "\\log p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) = \\log p(\\mathbf{ y}_{i, :}|\\mathbf{W},\n",
        "\\mathbf{ z}_{i, :}, \\sigma^2) + \\log p(\\mathbf{ z}_{i, :}) + \\text{const}\n",
        "$$ where the constant is not dependent on $\\mathbf{ z}$. As before we\n",
        "collect the quadratic terms in $\\mathbf{ z}_{i, :}$ and we assemble them\n",
        "into a Gaussian density over $\\mathbf{ z}$. $$\n",
        "\\log p(\\mathbf{ z}_{i, :} | \\mathbf{ y}_{i, :}) =\n",
        "-\\frac{1}{2\\sigma^2} (\\mathbf{ y}_{i, :} - \\mathbf{W}\\mathbf{ z}_{i,\n",
        ":})^\\top(\\mathbf{ y}_{i, :} - \\mathbf{W}\\mathbf{ z}_{i, :}) - \\frac{1}{2}\n",
        "\\mathbf{ z}_{i, :}^\\top \\mathbf{ z}_{i, :} + \\text{const}\n",
        "$$"
      ],
      "id": "oGXTQqWcH6MN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GjiwvK9H6MO"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Multiply out the terms in the brackets. Then collect the quadratic term\n",
        "and the linear terms together. Show that the posterior has the form $$\n",
        "\\mathbf{ z}_{i, :} | \\mathbf{W}\\sim \\mathscr{N}\\left(\\boldsymbol{ \\mu}_x,\\mathbf{C}_x\\right)\n",
        "$$ where $$\n",
        "\\mathbf{C}_x = \\left(\\sigma^{-2}\n",
        "\\mathbf{W}^\\top\\mathbf{W}+ \\mathbf{I}\\right)^{-1}\n",
        "$$ and $$\n",
        "\\boldsymbol{ \\mu}_x\n",
        "= \\mathbf{C}_x \\sigma^{-2}\\mathbf{W}^\\top \\mathbf{ y}_{i, :}\n",
        "$$ Compare this to the posterior for the Bayesian linear regression from\n",
        "last week, do they have similar forms? What matches and what differs?"
      ],
      "id": "8GjiwvK9H6MO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONhLsl9QH6MP"
      },
      "source": [
        "### Exercise 4 Answer\n",
        "\n",
        "Write your answer to Exercise 4 here"
      ],
      "id": "ONhLsl9QH6MP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnSSMyaqH6MQ"
      },
      "source": [
        "## Python Implementation of the Posterior\n",
        "\n",
        "Now let’s implement the system in code."
      ],
      "id": "vnSSMyaqH6MQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZCdA7CH6MQ"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "Use the values for $\\mathbf{W}$ and $\\sigma^2$ you have computed, along\n",
        "with the data set $\\mathbf{Y}$ to compute the posterior density over\n",
        "$\\mathbf{Z}$. Write a function of the form\n",
        "\n",
        "``` python\n",
        "mu_x, C_x = posterior(Y, W, sigma2)\n",
        "```\n",
        "\n",
        "where `mu_x` and `C_x` are the posterior mean and posterior covariance\n",
        "for the given $\\mathbf{Y}$.\n",
        "\n",
        "Don’t forget to subtract the mean of the data `Y` inside your function\n",
        "before computing the posterior: remember we assumed at the beginning of\n",
        "our analysis that the data had been centred (i.e. the mean was removed)."
      ],
      "id": "XcZCdA7CH6MQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcBnfxVCH6MS"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 5 here\n",
        "\n",
        "\n",
        "# Answer Code\n",
        "# Write code for you answer to this exercise in this box\n",
        "# Do not delete these comments, otherwise you will get zero for this answer.\n",
        "# Make sure your code has run and the answer is correct *before* submitting your notebook for marking.\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "def posterior(Y, W, sigma2):\n",
        "    Y_cent = Y - Y.mean(0)\n",
        "    # Compute posterior over X\n",
        "    C_x =\n",
        "    mu_x =\n",
        "    return mu_x, C_x\n",
        "\n"
      ],
      "id": "FcBnfxVCH6MS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UExjd99TH6MU"
      },
      "source": [
        "## Numerically Stable and Efficient Version\n",
        "\n",
        "Just as we saw for and computation of a matrix such as\n",
        "$\\mathbf{Y}^\\top\\mathbf{Y}$ (or its centred version) can be a bad idea\n",
        "in terms of loss of numerical accuracy. Fortunately, we can find the\n",
        "eigenvalues and eigenvectors of the matrix $\\mathbf{Y}^\\top\\mathbf{Y}$\n",
        "without direct computation of the matrix. This can be done with the\n",
        "[*singular value\n",
        "decomposition*](http://en.wikipedia.org/wiki/Singular_value_decomposition).\n",
        "The singular value decompsition takes a matrix, $\\mathbf{Z}$ and\n",
        "represents it in the form, $$\n",
        "\\mathbf{Z} = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top\n",
        "$$ where $\\mathbf{U}$ is a matrix of orthogonal vectors in the columns,\n",
        "meaning $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$. It has the same number\n",
        "of rows and columns as $\\mathbf{Z}$. The matrices $\\mathbf{\\Lambda}$ and\n",
        "$\\mathbf{V}$ are both square with dimensionality given by the number of\n",
        "columns of $\\mathbf{Z}$. The matrix $\\mathbf{\\Lambda}$ is *diagonal* and\n",
        "$\\mathbf{V}$ is an orthogonal matrix so\n",
        "$\\mathbf{V}^\\top\\mathbf{V} = \\mathbf{V}\\mathbf{V}^\\top = \\mathbf{I}$.\n",
        "The eigenvalues of the matrix $\\mathbf{Y}^\\top\\mathbf{Y}$ are then given\n",
        "by the singular values of the matrix $\\mathbf{Y}^\\top$ squared and the\n",
        "eigenvectors are given by $\\mathbf{U}$."
      ],
      "id": "UExjd99TH6MU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfug0AkrH6MV"
      },
      "source": [
        "## Solution for $\\mathbf{W}$\n",
        "\n",
        "Given the singular value decomposition of $\\mathbf{Y}$ then we have $$\n",
        "\\mathbf{W}=\n",
        "\\mathbf{U}\\mathbf{L}\\mathbf{R}^\\top\n",
        "$$ where $\\mathbf{R}$ is an arbitrary rotation matrix. This implies that\n",
        "the posterior is given by $$\n",
        "\\mathbf{C}_x =\n",
        "\\left[\\sigma^{-2}\\mathbf{R}\\mathbf{L}^2\\mathbf{R}^\\top + \\mathbf{I}\\right]^{-1}\n",
        "$$ because $\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}$. Since, by\n",
        "convention, we normally take $\\mathbf{R} = \\mathbf{I}$ to ensure that\n",
        "the principal components are orthonormal we can write $$\n",
        "\\mathbf{C}_x = \\left[\\sigma^{-2}\\mathbf{L}^2 +\n",
        "\\mathbf{I}\\right]^{-1}\n",
        "$$ which implies that $\\mathbf{C}_x$ is actually diagonal with elements\n",
        "given by $$\n",
        "c_i = \\frac{\\sigma^2}{\\sigma^2 + \\ell^2_i}\n",
        "$$ and allows us to write $$\n",
        "\\boldsymbol{ \\mu}_x = [\\mathbf{L}^2 + \\sigma^2\n",
        "\\mathbf{I}]^{-1} \\mathbf{L} \\mathbf{U}^\\top \\mathbf{ y}_{i, :}\n",
        "$$ $$\n",
        "\\boldsymbol{ \\mu}_x = \\mathbf{D}\\mathbf{U}^\\top \\mathbf{ y}_{i, :}\n",
        "$$ where $\\mathbf{D}$ is a diagonal matrix with diagonal elements given\n",
        "by $d_{i} = \\frac{\\ell_i}{\\sigma^2 + \\ell_i^2}$."
      ],
      "id": "Wfug0AkrH6MV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QRwZaJwH6Me"
      },
      "outputs": [],
      "source": [
        "import scipy as sp\n",
        "import numpy as np"
      ],
      "id": "4QRwZaJwH6Me"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaeVk6FoH6Mh"
      },
      "outputs": [],
      "source": [
        "# probabilistic PCA algorithm using SVD\n",
        "def ppca(Y, q, center=True):\n",
        "    \"\"\"Probabilistic PCA through singular value decomposition\"\"\"\n",
        "    # remove mean\n",
        "    if center:\n",
        "        Y_cent = Y - Y.mean(0)\n",
        "    else:\n",
        "        Y_cent = Y\n",
        "\n",
        "    # Comute singluar values, discard 'R' as we will assume orthogonal\n",
        "    U, sqlambd, _ = sp.linalg.svd(Y_cent.T,full_matrices=False)\n",
        "    lambd = (sqlambd**2)/Y.shape[0]\n",
        "    # Compute residual and extract eigenvectors\n",
        "    sigma2 = np.sum(lambd[q:])/(Y.shape[1]-q)\n",
        "    ell = np.sqrt(lambd[:q]-sigma2)\n",
        "    return U[:, :q], ell, sigma2\n",
        "\n",
        "def posterior(Y, U, ell, sigma2, center=True):\n",
        "    \"\"\"Posterior computation for the latent variables given the eigendecomposition.\"\"\"\n",
        "    if center:\n",
        "        Y_cent = Y - Y.mean(0)\n",
        "    else:\n",
        "        Y_cent = Y\n",
        "    C_x = np.diag(sigma2/(sigma2+ell**2))\n",
        "    d = ell/(sigma2+ell**2)\n",
        "    mu_x = np.dot(Y_cent, U)*d[None, :]\n",
        "    return mu_x, C_x"
      ],
      "id": "xaeVk6FoH6Mh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeWAB7NLH6Ml"
      },
      "source": [
        "## Scikit-learn implementation PCA\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/oil-flow-sklearn-pca.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/oil-flow-sklearn-pca.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We’ve implemented PCA as part of supporting the learning process, but in\n",
        "practice we can use the `scikit-learn` implementation. Let’s try it on\n",
        "the oil flow data."
      ],
      "id": "AeWAB7NLH6Ml"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7Y3kRzbH6Mm"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn"
      ],
      "id": "z7Y3kRzbH6Mm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGtpAFzvH6Mm"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "id": "tGtpAFzvH6Mm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtbRhk3HH6Mn"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)"
      ],
      "id": "ZtbRhk3HH6Mn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD_avU24H6Mo"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import mlai\n",
        "from mlai import plot"
      ],
      "id": "aD_avU24H6Mo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCEF5WB1H6Mp"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=plot.big_figsize)\n",
        "# Three labels stored in Y\n",
        "labels = [\"homogeneous\", \"annular\", \"stratified\"]\n",
        "for i in range(3):\n",
        "    ax.scatter(X_pca[y==i, 0], X_pca[y==i, 1], label=f'Label {i}')\n",
        "ax.set_xlabel('First principal component')\n",
        "ax.set_ylabel('Second principal component')\n",
        "ax.legend()\n",
        "\n",
        "plot.write_figure(\"oil-flow-pca-sklearn\", directory='./dimred')"
      ],
      "id": "UCEF5WB1H6Mp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLjKlSroH6Mp"
      },
      "source": [
        "Figure: <i>PCA of the oil flow data.</i>"
      ],
      "id": "mLjKlSroH6Mp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROEIkHljH6Mq"
      },
      "source": [
        "## Examples: Motion Capture Data\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/mocap-ppca.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/mocap-ppca.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "For our first example we’ll consider some motion capture data of a man\n",
        "breaking into a run. [Motion capture\n",
        "data](http://en.wikipedia.org/wiki/Motion_capture) involves capturing a\n",
        "3-d point cloud to represent a character, often by an underlying\n",
        "skeleton. For this data set, from Ohio State University, we have 54\n",
        "frame of motion capture, each frame containing 102 values, which are the\n",
        "3-d locations of 34 different points from the subject’s skeleton."
      ],
      "id": "ROEIkHljH6Mq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUEsUUFPH6Mq"
      },
      "outputs": [],
      "source": [
        "import pods"
      ],
      "id": "zUEsUUFPH6Mq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJAhPghfH6Mr"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.osu_run1()\n",
        "Y = data['Y']"
      ],
      "id": "qJAhPghfH6Mr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM6uG0oAH6Mt"
      },
      "source": [
        "Once the data is loaded in we can examine the first two principal\n",
        "components as follows,"
      ],
      "id": "gM6uG0oAH6Mt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k9zFP58H6Mu"
      },
      "outputs": [],
      "source": [
        "q = 2\n",
        "U, ell, sigma2 = ppca(Y, q)\n",
        "mu_x, C_x = posterior(Y, U, ell, sigma2)"
      ],
      "id": "3k9zFP58H6Mu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iYnke_UH6Mv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "id": "0iYnke_UH6Mv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHfC51slH6Mx"
      },
      "outputs": [],
      "source": [
        "plt.plot(mu_x[:, 0], mu_x[:, 1], 'rx-')"
      ],
      "id": "eHfC51slH6Mx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDaIqjVIH6Mx"
      },
      "source": [
        "Here because the data is a time course, we have connected points that\n",
        "are neighbouring in time. This highlights the form of the run, which\n",
        "involves 3 paces. This projects in our low dimensional space to 3 loops.\n",
        "We can examin how much residual variance there is in the system by\n",
        "looking at `sigma2`."
      ],
      "id": "YDaIqjVIH6Mx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qf3_fbXH6My"
      },
      "outputs": [],
      "source": [
        "print(sigma2)"
      ],
      "id": "5qf3_fbXH6My"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgHgyS02H6Mz"
      },
      "source": [
        "## Robot Navigation Example\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/robot-wireless-ppca.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/robot-wireless-ppca.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "In the next example we will load in data from a robot navigation\n",
        "problem. The data consists of wireless access point strengths as\n",
        "recorded by a robot performing a loop around the University of\n",
        "Washington’s Computer Science department in Seattle. The robot records\n",
        "all the wireless access points it can cache and stores their signal\n",
        "strength."
      ],
      "id": "fgHgyS02H6Mz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xxNdVZ0H6M0"
      },
      "outputs": [],
      "source": [
        "import pods"
      ],
      "id": "0xxNdVZ0H6M0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UqSFi-QH6M0"
      },
      "outputs": [],
      "source": [
        "data = pods.datasets.robot_wireless()\n",
        "Y = data['Y']\n",
        "Y.shape"
      ],
      "id": "5UqSFi-QH6M0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wy7KDfjH6M1"
      },
      "source": [
        "There are 215 observations of 30 different access points. In this case\n",
        "the model is suggesting that the access point signal strength should be\n",
        "linearly dependent on the location in the map. In other words we are\n",
        "expecting the access point strength for the $j$th access point at robot\n",
        "position $x_{i, :}$ to be represented by\n",
        "$y_{i, j} = \\mathbf{ w}_{j, :}^\\top \\mathbf{ z}_{i, :} + \\epsilon_{i,j}$."
      ],
      "id": "2Wy7KDfjH6M1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBV6VPX9H6M1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlai"
      ],
      "id": "ZBV6VPX9H6M1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng0dh7l-H6M2"
      },
      "outputs": [],
      "source": [
        "q = 2\n",
        "U, ell, sigma2 = ppca(Y, q)\n",
        "mu_x, C_x = posterior(Y, U, ell, sigma2)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "ax.plot(mu_x[:, 0], mu_x[:, 1], 'rx-')\n",
        "ax.set_title('Latent Variable: Robot Inferred Locations')\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "W = U*ell[None, :]\n",
        "ax.plot(W[:, 0], W[:, 1], 'bo')\n",
        "ax.set_title('Access Point Inferred Locations')\n",
        "mlai.write_figure(\"dem_robot_wireless_pca.svg\", directory=\"dimred\")"
      ],
      "id": "Ng0dh7l-H6M2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8TBswSxH6M3"
      },
      "outputs": [],
      "source": [
        "U, ell, sigma2 = ppca(Y.T, q)"
      ],
      "id": "c8TBswSxH6M3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do4_DCTNH6M4"
      },
      "source": [
        "# Interpretations of Principal Component Analysis\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-interpretations.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-interpretations.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>"
      ],
      "id": "Do4_DCTNH6M4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iftXl8OH6M4"
      },
      "source": [
        "## Relationship to Matrix Factorization\n",
        "\n",
        "We can use the robot naviation example to realise that PCA (and factor\n",
        "analysis) are very reminiscient of the that we used for introducing\n",
        "objective functions. In that system we used slightly different notation,\n",
        "$\\mathbf{u}_{i, :}$ for *user* location in our metaphorical library and\n",
        "$\\mathbf{v}_{j, :}$ for *item* location in our metaphorical library. To\n",
        "see how these systems are somewhat analagous, now let us think about the\n",
        "user as the robot and the items as the wifi access points. We can plot\n",
        "the relative location of both. This process is known as “SLAM”:\n",
        "simultaneous *localisation* and *mapping*. A latent variable model of\n",
        "the type we have developed is one way of performing SLAM. We have an\n",
        "estimate of the *landmarks* in the system (in this case WIFI access\n",
        "points) and we have an estimate of the robot position. These are\n",
        "analagous to the estimate of the user’s position and the estimate of the\n",
        "items positions in the library. In the matrix factorisation example\n",
        "users are informing us what items they are ‘close’ to by expressing\n",
        "their preferences, in the robot localization example the robot is\n",
        "informing us what access point it is close to by measuring signal\n",
        "strength.\n",
        "\n",
        "From a personal perspective, I find this analogy quite comforting. I\n",
        "think it is very arguable that one of the mechanisms through which we\n",
        "(as humans) may have developed higher reasoning is through the need to\n",
        "navigate around our environment, identifying landmarks and associating\n",
        "them with our search for food. If such a system were to exist, the idea\n",
        "that it could be readily adapted to other domains such as categorising\n",
        "the nature of the different foodstuffs we were able to forage is\n",
        "intriguing.\n",
        "\n",
        "From an algorithmic perspective, we also can now realise that matrix\n",
        "factorization and latent variable modelling are effectively the same\n",
        "thing. The only difference is the objective function and our\n",
        "probabilistic (or lack of probabilistic) treatment of the variables. But\n",
        "the prediction function for both systems, $$\n",
        "f_{i, j} =\n",
        "\\mathbf{u}_{i, :}^\\top \\mathbf{v}_{j, :}\n",
        "$$ for matrix factorization or $$\n",
        "f_{i, j} = \\mathbf{ z}_{i, :}^\\top \\mathbf{ w}_{j, :}\n",
        "$$ for probabilistic PCA and factor analysis are the same."
      ],
      "id": "2iftXl8OH6M4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBNviw8H6M5"
      },
      "source": [
        "## Other Interpretations of PCA: Separating Model and Algorithm\n",
        "\n",
        "Since Hotelling first introduced his perspective on factor analysis as\n",
        "PCA there has been somewhat of a conflation of the idea of the model\n",
        "underlying PCA (for which it was very clear that Hotelling was inspired\n",
        "by Factor Analysis) and the algorithm that is used to fit that model:\n",
        "the eigenvalues and eigenvectors of the covariance matrix. The\n",
        "eigenvectors of an ellipsoid have been known since the middle of the\n",
        "19th century as the principal axes of the elipsoid, and they arise\n",
        "through the following additional ideas: seeking the orthogonal\n",
        "directions of *maximum variance* in a dataset. Pearson in 1901 arrived\n",
        "at the same algorithm driven by a desire to seek a *symmetric\n",
        "regression* between two covariate/response variables $x$ and $y$\n",
        "(**Pearson:01?**). He is, therefore, often credited with the invention\n",
        "of principal component analysis, but to me this seems disengenous. His\n",
        "aim was very different from Hotellings, it was just happened that the\n",
        "optimal solution for his model was coincident with that of Hotelling.\n",
        "The approach is also known as the [Karhunen Loeve\n",
        "Transform](http://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem)\n",
        "in stochastic process theory and in classical multidimensional scaling\n",
        "the same operation can be shown to be minimising a particular objective\n",
        "function based on interpoint distances in the data and the latent space\n",
        "(see the section on Classical Multidimensional Scaling in [Mardia, Kent\n",
        "and\n",
        "Bibby](http://store.elsevier.com/Multivariate-Analysis/Kanti-%20Mardia/isbn-9780124712522/))\n",
        "(Mardia et al., 1979). One of my own contributions to machine learning\n",
        "was deriving yet another model whose linear variant was solved by\n",
        "finding the principal subspace of the covariance matrix (an approach I\n",
        "termed dual probabilistic PCA or probabilistic principal coordinate\n",
        "analysis). Finally, the approach is sometimes referred to simply as\n",
        "singular value decomposition (SVD). The singular value decomposition of\n",
        "a data set has the following form, $$\n",
        "\\mathbf{Y}= \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{U}^\\top\n",
        "$$ where $\\mathbf{V}\\in\\Re^{n\\times n}$ and\n",
        "$\\mathbf{U}^\\in \\Re^{p\\times p}$ are square orthogonal matrices and\n",
        "$\\mathbf{\\Lambda}^{n \\times p}$ is zero apart from its first $p$\n",
        "diagonal entries. Singularvalue decomposition gives a diagonalisation of\n",
        "the covariance matrix, because under the SVD we have $$\n",
        "\\mathbf{Y}^\\top\\mathbf{Y}=\n",
        "\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top\\mathbf{V} \\boldsymbol{\\Lambda}\n",
        "\\mathbf{U}^\\top = \\mathbf{U}\\boldsymbol{\\Lambda}^2 \\mathbf{U}^\\top\n",
        "$$ where $\\boldsymbol{\\Lambda}^2$ is now the eigenvalues of the\n",
        "covariane matrix and $\\mathbf{U}$ are the eigenvectors. So performing\n",
        "the SVD can simply be seen as another approach to determining the\n",
        "principal components."
      ],
      "id": "dQBNviw8H6M5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Lu3ti8H6M6"
      },
      "source": [
        "## Separating Model and Algorithm\n",
        "\n",
        "I’ve given a fair amount of personal thought to this situation and my\n",
        "own opinion that this confusion about method arises because of a\n",
        "conflation of model and algorithm. The model of Hotelling, that which he\n",
        "termed principal component analysis, was really a variant of factor\n",
        "analysis, and it was unfortunate that he chose to rename it. However,\n",
        "the algorithm he derived was a very convenient way of optimising a\n",
        "(simplified) factor analysis, and it’s therefore become very popular.\n",
        "The algorithm is also the optimal solution for many other models of the\n",
        "data, even some which might seem initally to be unrelated (e.g. seeking\n",
        "directions of maximum variance). It is only through the mathematics of\n",
        "this linear system (which also contains some intersting symmetries) that\n",
        "all these ides become related. However, as soon as we choose to\n",
        "non-linearise the system (e.g. through basis functions) we find that\n",
        "each of the non-linear intepretations we can derive for the different\n",
        "models each leads to a very different algorithm (if such an algorithm is\n",
        "possible). For example [principal\n",
        "curves](http://web.stanford.edu/~hastie/Papers/Principal_Curves.pdf) of\n",
        "Hastie and Stuetzle (1989) attempt to non-linearise the maximum variance\n",
        "interpretation, [kernel\n",
        "PCA](http://en.wikipedia.org/wiki/Kernel_principal_component_analysis)\n",
        "of Schölkopf et al. (1998) uses basis functions to form the eigenvalue\n",
        "problem in a nonlinear space, and my own work in this area\n",
        "[non-linearises the dual probabilistic\n",
        "PCA](http://jmlr.org/papers/volume6/lawrence05a/lawrence05a.pdf)\n",
        "(Lawrence, 2005).\n",
        "\n",
        "My conclusion is that when you are doing machine learning you should\n",
        "always have it clear in your mind what your *model* is and what your\n",
        "*algorithm* is. You can recognise your model because it normally\n",
        "contains a prediction function and an objective function. The algorithm\n",
        "on the other hand is the sequence of steps you implement on the computer\n",
        "to solve for the parameters of this model. For efficient implementation,\n",
        "we often modify our model to allow for faster algorithms, and this is a\n",
        "perfectly valid pragmatist’s approach, so conflation of model and\n",
        "algorithm is not always a bad thing. But for clarity of thinking and\n",
        "understanding it is necessary to maintain the separation and to maintain\n",
        "a handle on when and why we perform the conflation."
      ],
      "id": "T3Lu3ti8H6M6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saCUTmI2H6M7"
      },
      "source": [
        "# PCA in Practice\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/pca-in-practice.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/pca-in-practice.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Principal component analysis is so effective in practice that there has\n",
        "almost developed a mini-industry in renaming the method itself (which is\n",
        "ironic, given its origin). In particular [Latent Semantic\n",
        "Indexing](http://en.wikipedia.org/wiki/Latent_semantic_indexing) in text\n",
        "processing is simply PCA on a particular representation of the term\n",
        "frequencies of the document. There is a particular fad to rename the\n",
        "eigenvectors after the nature of the data you are examining, perhaps\n",
        "initially triggered by [Turk and\n",
        "Pentland’s](http://www.face-rec.org/algorithms/PCA/jcn.pdf) paper on\n",
        "eigenfaces, but also with\n",
        "[eigenvoices](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester1_2007_8/kuhn-%20junqua-eigenvoice-icslp1998.pdf)\n",
        "and [eigengenes](http://www.biomedcentral.com/1752-0509/1/54). This\n",
        "seems to be an instantiation of a wider, and hopefully subconcious,\n",
        "tendency in academia to attempt to differentiate one idea from the same\n",
        "idea in related fields in order to emphasise the novelty. The\n",
        "unfortunate result is somewhat of a confusing literature for relatively\n",
        "simple model. My recommendations would be as follows. If you have\n",
        "multivariate data, applying some form of principal component would seem\n",
        "to be a very good idea as a first step. Even if you intend to later\n",
        "perform classification or regression on your data, it can give you\n",
        "understanding of the structure of the underlying data and help you to\n",
        "develop your intuitions about the nature of your data. Intelligent\n",
        "plotting and interaction with your data is always a good think, and for\n",
        "high dimensional data that means that you need some way of\n",
        "visualisation, PCA is typically a good starting point."
      ],
      "id": "saCUTmI2H6M7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJL5ju8mH6M7"
      },
      "source": [
        "# PPCA Marginal Likelihood\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-marginal-likelihood.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_dimred/includes/ppca-marginal-likelihood.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We have developed the posterior density over the latent variables given\n",
        "the data and the parameters, and due to symmetries in the underlying\n",
        "prediction function, it has a very similar form to its sister density,\n",
        "the posterior of the weights given the data from Bayesian regression.\n",
        "Two key differences are as follows. If we were to do a Bayesian multiple\n",
        "output regression we would find that the marginal likelihood of the data\n",
        "is independent across the features and correlated across the data, $$\n",
        "p(\\mathbf{Y}|\\mathbf{Z})\n",
        "= \\prod_{j=1}^p \\mathscr{N}\\left(\\mathbf{ y}_{:, j}|\\mathbf{0},\n",
        "\\alpha\\mathbf{Z}\\mathbf{Z}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
        "$$ where $\\mathbf{ y}_{:, j}$ is a column of the data matrix and the\n",
        "independence is across the *features*, in probabilistic PCA the marginal\n",
        "likelihood has the form, $$\n",
        "p(\\mathbf{Y}|\\mathbf{W}) = \\prod_{i=1}^n\\mathscr{N}\\left(\\mathbf{ y}_{i,\n",
        ":}|\\mathbf{0},\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right)\n",
        "$$ where $\\mathbf{ y}_{i, :}$ is a row of the data matrix $\\mathbf{Y}$\n",
        "and the independence is across the data points."
      ],
      "id": "ZJL5ju8mH6M7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsM4dUsZH6M8"
      },
      "source": [
        "# Computation of the Log Likelihood\n",
        "\n",
        "The quality of the model can be assessed using the log likelihood of\n",
        "this Gaussian form. $$\n",
        "\\log p(\\mathbf{Y}|\\mathbf{W}) = -\\frac{n}{2} \\log \\left|\n",
        "\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right| -\\frac{1}{2}\n",
        "\\sum_{i=1}^n\\mathbf{ y}_{i, :}^\\top \\left(\\mathbf{W}\\mathbf{W}^\\top + \\sigma^2\n",
        "\\mathbf{I}\\right)^{-1} \\mathbf{ y}_{i, :} +\\text{const}\n",
        "$$ but this can be computed more rapidly by exploiting the low rank form\n",
        "of the covariance covariance,\n",
        "$\\mathbf{C}= \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}$ and the\n",
        "fact that $\\mathbf{W}= \\mathbf{U}\\mathbf{L}\\mathbf{R}^\\top$.\n",
        "Specifically, we first use the decomposition of $\\mathbf{W}$ to write:\n",
        "$$\n",
        "-\\frac{n}{2} \\log \\left| \\mathbf{W}\\mathbf{W}^\\top + \\sigma^2 \\mathbf{I}\\right|\n",
        "= -\\frac{n}{2} \\sum_{i=1}^q \\log (\\ell_i^2 + \\sigma^2) - \\frac{n(p-q)}{2}\\log\n",
        "\\sigma^2,\n",
        "$$ where $\\ell_i$ is the $i$th diagonal element of $\\mathbf{L}$. Next,\n",
        "we use the [Woodbury matrix\n",
        "identity](http://en.wikipedia.org/wiki/Woodbury_matrix_identity) which\n",
        "allows us to write the inverse as a quantity which contains another\n",
        "inverse in a smaller matrix: $$\n",
        "(\\sigma^2 \\mathbf{I}+ \\mathbf{W}\\mathbf{W}^\\top)^{-1} =\n",
        "\\sigma^{-2}\\mathbf{I}-\\sigma^{-4}\\mathbf{W}{\\underbrace{(\\mathbf{I}+\\sigma^{-2}\\mathbf{W}^\\top\\mathbf{W})}_{\\mathbf{C}_x}}^{-1}\\mathbf{W}^\\top\n",
        "$$ So, it turns out that the original inversion of the $p \\times p$\n",
        "matrix can be done by forming a quantity which contains the inversion of\n",
        "a $q \\times q$ matrix which, moreover, turns out to be the quantity\n",
        "$\\mathbf{C}_x$ of the posterior.\n",
        "\n",
        "Now, we put everything together to obtain: $$\n",
        "\\log p(\\mathbf{Y}|\\mathbf{W}) = -\\frac{n}{2} \\sum_{i=1}^q\n",
        "\\log (\\ell_i^2 + \\sigma^2)\n",
        "- \\frac{n(p-q)}{2}\\log \\sigma^2 - \\frac{1}{2} \\text{tr}\\left(\\mathbf{Y}^\\top \\left(\n",
        "\\sigma^{-2}\\mathbf{I}-\\sigma^{-4}\\mathbf{W}\\mathbf{C}_x\n",
        "\\mathbf{W}^\\top \\right) \\mathbf{Y}\\right) + \\text{const},\n",
        "$$ where we used the fact that a scalar sum can be written as\n",
        "$\\sum_{i=1}^n\\mathbf{ y}_{i,:}^\\top \\mathbf{K}\\mathbf{ y}_{i,:} = \\text{tr}\\left(\\mathbf{Y}^\\top \\mathbf{K}\\mathbf{Y}\\right)$,\n",
        "for any matrix $\\mathbf{K}$ of appropriate dimensions. We now use the\n",
        "properties of the trace\n",
        "$\\text{tr}\\left(\\mathbf{A}+\\mathbf{B}\\right)=\\text{tr}\\left(\\mathbf{A}\\right)+\\text{tr}\\left(\\mathbf{B}\\right)$\n",
        "and\n",
        "$\\text{tr}\\left(c \\mathbf{A}\\right) = c \\text{tr}\\left(\\mathbf{A}\\right)$,\n",
        "where $c$ is a scalar and $\\mathbf{A},\\mathbf{B}$ matrices of compatible\n",
        "sizes. Therefore, the final log likelihood takes the form: $$\n",
        "\\log p(\\mathbf{Y}|\\mathbf{W}) = -\\frac{n}{2}\n",
        "\\sum_{i=1}^q \\log (\\ell_i^2 + \\sigma^2) - \\frac{n(p-q)}{2}\\log \\sigma^2 -\n",
        "\\frac{\\sigma^{-2}}{2} \\text{tr}\\left(\\mathbf{Y}^\\top \\mathbf{Y}\\right)\n",
        "+\\frac{\\sigma^{-4}}{2} \\text{tr}\\left(\\mathbf{B}\\mathbf{C}_x\\mathbf{B}^\\top\\right) +\n",
        "\\text{const}\n",
        "$$ where we also defined $\\mathbf{B}=\\mathbf{Y}^\\top\\mathbf{W}$.\n",
        "Finally, notice that\n",
        "$\\text{tr}\\left(\\mathbf{Y}\\mathbf{Y}^\\top\\right)=\\text{tr}\\left(\\mathbf{Y}^\\top\\mathbf{Y}\\right)$\n",
        "can be computed faster as the sum of all the elements of\n",
        "$\\mathbf{Y}\\circ\\mathbf{Y}$, where $\\circ$ denotes the element-wise (or\n",
        "[Hadamard](http://en.wikipedia.org/wiki/Hadamard_product_(matrices)))\n",
        "product."
      ],
      "id": "UsM4dUsZH6M8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3mjdA_5H6M9"
      },
      "source": [
        "## Summary and Key Points\n",
        "\n",
        "We’ve covered several key ideas about dimensionality reduction:\n",
        "\n",
        "1.  High-dimensional spaces have counter-intuitive properties:\n",
        "    -   The curse of dimensionality\n",
        "    -   Concentration of distances\n",
        "2.  Real data doesn’t behave like random high-dimensional data because:\n",
        "    -   It lies near lower-dimensional manifolds\n",
        "    -   It has structure imposed by physics, biology, or other\n",
        "        constraints\n",
        "3.  This structure makes dimensionality reduction possible:\n",
        "    -   PCA finds linear manifolds\n",
        "    -   More sophisticated methods can find nonlinear manifolds\n",
        "4.  The probabilistic perspective helps us:\n",
        "    -   Understand when methods will work\n",
        "    -   Quantify uncertainty in our reduced representations\n",
        "    -   Connect dimensionality reduction to other machine learning\n",
        "        approaches"
      ],
      "id": "S3mjdA_5H6M9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_QcMLb_H6M-"
      },
      "source": [
        "## Further Reading\n",
        "\n",
        "-   Chapter 7 up to pg 249 of Rogers and Girolami (2011)"
      ],
      "id": "3_QcMLb_H6M-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xIslgNiH6NA"
      },
      "source": [
        "## Thanks!\n",
        "\n",
        "For more information on these subjects and more you might want to check\n",
        "the following resources.\n",
        "\n",
        "-   company: [Trent AI](https://trent.ai)\n",
        "-   book: [The Atomic\n",
        "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
        "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
        "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
        "-   newspaper: [Guardian Profile\n",
        "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
        "-   blog:\n",
        "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
      ],
      "id": "5xIslgNiH6NA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWTB7uyzH6ND"
      },
      "source": [
        "::: {.cell .markdown}\n",
        "\n",
        "## References\n",
        "\n",
        "Bishop, C.M., James, G.D., 1993. Analysis of multiphase flows using\n",
        "dual-energy gamma densitometry and neural networks. Nuclear Instruments\n",
        "and Methods in Physics Research A327, 580–593.\n",
        "<https://doi.org/10.1016/0168-9002(93)90728-Z>\n",
        "\n",
        "Hastie, T., Stuetzle, W., 1989. Principal curves. Journal of the\n",
        "American Statistical Association 84, 502–516.\n",
        "\n",
        "Hotelling, H., 1933. Analysis of a complex of statistical variables into\n",
        "principal components. Journal of Educational Psychology 24, 417–441.\n",
        "\n",
        "Lawrence, N.D., 2005. Probabilistic non-linear principal component\n",
        "analysis with Gaussian process latent variable models. Journal of\n",
        "Machine Learning Research 6, 1783–1816.\n",
        "\n",
        "Le Cun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E.,\n",
        "Hubbard, W., Jackel, L.D., 1989. Backpropagation applied to handwritten\n",
        "zip code recognition. Neural Computation 1, 541–551.\n",
        "<https://doi.org/10.1162/neco.1989.1.4.541>\n",
        "\n",
        "Mardia, K.V., Kent, J.T., Bibby, J.M., 1979. Multivariate analysis.\n",
        "Academic Press, London.\n",
        "\n",
        "Ng, A.Y., Jordan, M.I., Weiss, Y., n.d. On spectral clustering: Analysis\n",
        "and an algorithm.\n",
        "\n",
        "Pearson, K., 1901. On lines and planes of closest fit to systems of\n",
        "points in space. The London, Edinburgh and Dublin Philosophical Magazine\n",
        "and Journal of Science, Sixth Series 2, 559–572.\n",
        "\n",
        "Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC\n",
        "Press.\n",
        "\n",
        "Roweis, S.T., n.d. EM algorithms for PCA and SPCA. pp. 626–632.\n",
        "\n",
        "Schölkopf, B., Smola, A., Müller, K.-R., 1998. Nonlinear component\n",
        "analysis as a kernel eigenvalue problem. Neural Computation 10,\n",
        "1299–1319. <https://doi.org/10.1162/089976698300017467>\n",
        "\n",
        "Shi, J., Malik, J., 2000. Normalized cuts and image segmentation. IEEE\n",
        "Transactions on Pattern Analysis and Machine Intelligence 22, 888–905.\n",
        "\n",
        "Spearman, C.E., 1904. \"General intelligence,\" objectively determined and\n",
        "measured. The American Journal of Psychology 15, 201–292.\n",
        "\n",
        "Tipping, M.E., Bishop, C.M., 1999b. Mixtures of probabilistic principal\n",
        "component analysers. Neural Computation 11, 443–482.\n",
        "\n",
        "Tipping, M.E., Bishop, C.M., 1999a. Probabilistic principal component\n",
        "analysis. Journal of the Royal Statistical Society, B 6, 611–622.\n",
        "<https://doi.org/doi:10.1111/1467-9868.00196>"
      ],
      "id": "ZWTB7uyzH6ND"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    }
  }
}